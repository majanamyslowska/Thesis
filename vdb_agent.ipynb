{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE A VECTOR, HYBRID DB based on the csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "import textwrap\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key='sk-e46gtJZjdqqgxbe9i9tPbMwRxEbuH3bcSd6lTJa9TMT3BlbkFJFXbBGoSo9k4ehTjaCF7l-Vl0wj4jBf1LpvkKN8E1sA',\n",
    ")\n",
    "\n",
    "class OpenAIEmbeddingModel:\n",
    "    def __init__(self, model_name=\"text-embedding-3-small\"):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        return client.embeddings.create(input = [text], model=self.model_name).data[0].embedding\n",
    "    \n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "embed_model = OpenAIEmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL version: ('PostgreSQL 14.12 (Homebrew) on x86_64-apple-darwin23.4.0, compiled by Apple clang version 15.0.0 (clang-1500.3.9.4), 64-bit',)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "db_name = \"vdb\" # change the db name here\n",
    "host = \"localhost\"\n",
    "password = \"password\"\n",
    "port = \"5432\"\n",
    "user = \"maja2\"\n",
    "\n",
    "# conn = psycopg2.connect(\n",
    "#     dbname=\"postgres\",\n",
    "#     host=host,\n",
    "#     password=password,\n",
    "#     port=port,\n",
    "#     user=user,\n",
    "# )\n",
    "# conn.autocommit = True\n",
    "\n",
    "# with conn.cursor() as c:\n",
    "#     c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "#     c.execute(f\"CREATE DATABASE {db_name}\")\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    c.execute(\"SELECT version();\")  # Get the version of the PostgreSQL server\n",
    "    version = c.fetchone()\n",
    "    print(\"Connected to PostgreSQL version:\", version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "\n",
    "connection_string = \"postgresql://postgres:password@localhost:5432\"\n",
    "\n",
    "hybrid_vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"llama2_paper\",\n",
    "    embed_dim=1536,\n",
    "    hybrid_search=True,\n",
    "    text_search_config=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n",
      "\n",
      "Table: data_llama2_paper\n",
      "Columns:\n",
      " - id: bigint\n",
      " - metadata_: json\n",
      " - embedding: USER-DEFINED\n",
      " - text_search_tsv: tsvector\n",
      " - text: character varying\n",
      " - node_id: character varying\n",
      "Number of rows: 27599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as c:\n",
    "    # Get a list of all tables in the public schema\n",
    "    c.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';\")\n",
    "    tables = c.fetchall()\n",
    "    print(\"Tables in the database:\\n\")\n",
    "    \n",
    "    # For each table, get the columns, their data types, and row count\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        print(f\"Table: {table_name}\")\n",
    "        \n",
    "        # Get the columns of the table\n",
    "        c.execute(f\"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '{table_name}';\")\n",
    "        columns = c.fetchall()\n",
    "        \n",
    "        print(\"Columns:\")\n",
    "        for column in columns:\n",
    "            print(f\" - {column[0]}: {column[1]}\")\n",
    "        \n",
    "        # Get the number of rows in the table\n",
    "        c.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "        row_count = c.fetchone()[0]\n",
    "        print(f\"Number of rows: {row_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample node from data_llama2_paper:\n",
      "id: 1\n",
      "metadata_: Title: US shale industry's $200bn dealmaking wave redraws energy landscape\n",
      "Date: June 2, 2024\n",
      "Dealmaking in US oil and gas has surged to almost $200bn in the past year as the biggest\n",
      "producers compete to swallow up rivals in a race for scale that has redrawn the national energy\n",
      "landscape.But as the country's best drilling acreage is snapped up, companies are casting a wider\n",
      "net and looking beyond the most sought-after oilfields for acquisitions that will bolster their ability to\n",
      "pump hydrocarbons in the years ahead.\"We are in the midst of a consolidation wave and I don't\n",
      "think it is over yet,\" said Jon Hughes, chief executive of Petrie Partners, a boutique investment\n",
      "banking firm that advised on Pioneer Natural Resources' $60bn sale to ExxonMobil.\"We've gone\n",
      "from about 65 to 41 publicly traded oil and gas companies in the US in less than five years.\"Since\n",
      "last July companies including Exxon, Chevron and Occidental Petroleum have announced $194bn\n",
      "worth of deals across the US shale patch, according to consultancy Rystad Energy. This is almost\n",
      "triple the amount in the previous 12-month period. The latest came this week when ConocoPhillips\n",
      "announced a $22.5bn acquisition of Marathon Oil, after talks between the companies were reported\n",
      "by Financial Times. At least another $62bn of assets are known to be on the market, according to\n",
      "Rystad. Companies including Permian Resources, Matador Resources, Chord Energy and Civitas\n",
      "Resources are in the sights of bigger players, said Michael Alfaro of Gallo Partners, a hedge fund\n",
      "focusing on industrials and energy. He also pointed to privately held companies including Double\n",
      "Eagle and Mewbourne Oil as attractive prospects.Houston-based EOG, valued at $70bn, and\n",
      "Oklahoma-based Devon Energy, valued at $30bn, are the biggest publicly listed US players yet to\n",
      "strike in the recent wave. Devon risks becoming a target for other players if it fails to bulk up,\n",
      "analysts said. The company had held talks with Marathon, but was beaten to the punch by Conoco,\n",
      "said people familiar with the deal.EOG and Devon did not respond to requests for comment on their\n",
      "plans.The dealmaking burst has entered a new phase. With much of the best acreage spoken for in\n",
      "the prolific Permian Basin of Texas and New Mexico -- the engine room of the country's oil industry\n",
      "-- companies are looking further afield. Consolidation has left almost two-thirds of the field's shale oil\n",
      "embedding: {'total_pages': 3, 'file_path': './data3/temp.pdf', 'source': '1', 'date': 'June 2, 2024', '_node_content': '{\"id_\": \"8cf852e2-b8b7-4d63-8a2b-38b6753b23ef\", \"embedding\": null, \"metadata\": {\"total_pages\": 3, \"file_path\": \"./data3/temp.pdf\", \"source\": \"1\", \"date\": \"June 2, 2024\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"\", \"mimetype\": \"text/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}', '_node_type': 'TextNode', 'document_id': 'None', 'doc_id': 'None', 'ref_doc_id': 'None'}\n",
      "text_search_tsv: 8cf852e2-b8b7-4d63-8a2b-38b6753b23ef\n",
      "text: [-0.033530857,0.0027895393,0.04182586,0.040196933,0.034483153,0.061247695,-0.0027268883,0.06340289,0.02445898,-0.011245868,0.052125696,-0.036212325,-0.045985892,-0.00080898206,0.013432391,0.06335277,-0.01019333,0.018895565,-0.03187687,-0.015863253,-0.009096936,-0.0056981156,0.0049463026,0.004247743,-0.00018609328,0.012931183,-0.034758817,0.029746732,0.008013073,0.0020408588,0.033255193,-0.040297173,0.031099996,-0.0016054339,-0.026037788,0.01922135,-0.0060771545,0.023381382,0.0073990924,-0.017742785,0.016201569,-0.030273002,-0.019672439,0.02502284,0.009729712,-0.0066974005,-0.042652853,-0.041048985,0.044482265,0.04425672,-0.045685165,-0.0012859134,-0.0071046324,-0.034583393,0.0016304944,-0.052476544,0.0581402,-0.0014018179,0.043354545,-0.032428198,0.006728726,-0.0068352325,0.018118693,0.025787184,0.007499334,0.018544719,-0.012079128,0.08685946,-0.01333215,0.027015146,0.0033580977,0.009247299,-0.06545784,0.04806591,0.055132948,-0.028368408,-0.025461398,0.013557693,-0.0010196463,-0.026313452,0.012016477,-0.016151449,-0.012429974,-0.018883035,-0.0004824133,0.012047802,-0.010644418,-0.027792018,-0.022930294,0.042552613,-0.03618726,-0.029320704,0.0028772508,-0.006747521,-0.029796854,0.010907553,0.017191457,0.008890188,0.032277837,-0.00086771743,-0.030122638,0.01627675,0.025737062,0.008808741,-0.01863243,0.001566277,0.015336984,-0.0180811,0.04460757,-0.040247053,-0.022454146,-0.013282029,0.0020408588,0.0072925854,-0.021063292,0.00957935,-0.030874452,-0.010017907,-0.010093089,0.024646934,-0.06159854,-0.01279335,0.011841054,0.051774852,0.0008066326,0.031024814,-0.023043066,0.0033737605,-0.025473928,-0.07006896,-0.016489765,0.02556164,0.02729081,-0.046061072,-0.02146426,-0.05989443,-0.057488628,0.024646934,0.0012029007,0.0360369,0.03779113,0.03490918,0.017016033,0.018883035,-0.06039564,-0.016564945,-0.021276306,-0.04801579,-0.017141335,-0.026062848,-0.03548557,-0.035184845,-0.009303685,0.008627053,0.045960832,-0.036688473,-0.01336974,-0.014798185,-0.024208376,-0.031074936,-0.042828277,0.036362685,-0.020649796,0.008652114,0.01925894,0.06976824,-0.025273444,-0.057037544,-0.06786365,-0.023945242,-0.025937546,-0.041449953,0.015575058,-0.043930937,-0.014672883,-0.06285156,-0.049469292,0.062600955,0.012718169,-0.01684061,-0.008595728,-0.02435874,-0.004814735,-0.02435874,-0.03721474,0.015311924,-0.03646293,0.038242217,0.020085936,0.0042007547,-0.007988012,0.025085492,0.02270475,0.005378595,0.006164866,-0.04468275,0.042001285,0.028117804,0.0014636858,-0.028568892,0.011646835,0.012436239,-0.006199324,-0.023343792,-0.035109665,0.012304672,0.009046815,-3.5057683e-06,0.01577554,0.009134527,0.026564058,0.034007005,0.01684061,0.09788605,0.014347097,-0.032829165,-0.048366632,0.01978521,-0.00027703523,0.010813576,0.0209004,0.036813773,0.009510433,0.044457205,-0.024759706,0.004463889,0.02034907,-0.019421834,0.009115732,-0.0066472795,-0.018294115,0.018457009,-0.005735706,-0.019998224,0.00928489,-0.033656158,0.029846974,-0.0005121726,0.0033549652,0.0011018758,-0.054180652,0.00092332024,-0.017617483,0.020211238,0.007787529,0.02443392,0.0055508856,-0.03185181,-0.011985151,0.012066598,0.014134083,-0.019609788,0.01981027,-0.01189744,0.020511962,-0.031801686,0.01979774,0.021100882,-0.028218046,0.03781619,0.0011057915,0.0569373,-0.044382025,0.035109665,-0.05232618,0.037415225,0.017730255,0.014459869,0.019409304,0.028268166,-0.008357654,0.010368753,-0.002783274,0.019597257,-0.031150116,-0.014873366,0.043780573,-0.032052293,0.006622219,0.021940408,0.014647822,0.0266643,0.011834789,0.019609788,0.037941493,-0.030373244,0.0636535,0.016702777,-0.0009327179,-0.05463174,0.048993144,0.025373686,0.022191012,-0.026488876,0.027591536,-0.013858419,-0.005688718,-0.045209017,-0.0036744857,-0.0030291795,0.011465147,-0.022378964,-0.0037026787,-0.021852696,-0.023105718,-0.010838636,0.03653811,-0.05874165,-0.028318288,0.004614252,-0.001583506,0.020173647,0.06340289,-0.06310216,-0.03082433,0.009942726,0.0056010066,0.002647008,-0.056636576,-0.024083074,0.018319175,-0.036312565,-0.003229663,0.021351488,0.020035814,-0.009065611,0.038718365,-0.025047902,0.007493069,-0.061297815,0.014748064,0.04758976,0.008175965,0.0050152186,-0.02151438,-0.047263972,0.03077421,-0.032904346,0.029320704,-0.025348626,-0.04012175,0.0052626906,0.07031957,-0.0021129076,-0.014585171,-0.002347849,0.027792018,0.04871748,-0.025987668,0.06831474,-0.040547777,0.033931825,-0.005560283,0.05352908,0.023932712,0.03022288,0.0061617335,-0.03598678,0.04468275,0.03383158,-0.037841253,-0.0019594126,0.02328114,-0.0015192886,0.02096305,0.013282029,-0.05994455,-0.0511734,0.056135368,0.013645405,0.075933106,0.03370628,-0.01747965,-0.0060082385,-0.053077992,-0.0022100168,0.01981027,0.049168568,0.02380741,0.047238912,0.0024809828,-0.0050183516,-0.024822358,0.014773124,-0.062099747,-0.010663213,-0.014848306,-0.035059545,-0.050747372,-0.048316512,0.03774101,0.017642545,0.0050810026,-0.00037316547,0.00901549,-0.010337428,-0.009053081,0.038041733,-0.008902718,0.021877756,-0.033305313,-0.005253293,-0.021013172,-0.069567755,0.0011183218,0.041475013,-0.021414138,0.003455207,-0.061347935,-0.062350355,0.053679444,6.405828e-06,0.032127474,0.009948991,-0.016953383,0.04413142,-0.004122441,0.0020486903,-0.0360369,0.0407232,0.06886607,0.0003757107,-0.017116275,-0.01076972,-0.047539636,-0.009980317,0.020612204,0.019497016,-0.025047902,-0.008934043,0.0012937448,0.022742342,0.028819496,0.010775985,0.023005476,-0.008351388,-0.02959637,0.029671552,-0.03150096,-0.064555675,-0.0026501406,0.00904055,0.019321593,-0.018682553,-0.06400434,-0.020724976,0.037966553,0.00436678,-0.019998224,-0.007016921,0.009880075,-0.0052063046,0.01973509,0.0060708895,-0.018457009,0.015662769,0.09397662,-0.01744206,-0.0034802675,0.07127187,0.013119136,-0.016163979,0.017341819,-0.007950421,-0.023519214,-0.02734093,-0.041700557,0.00013685344,0.020574614,0.0056918506,0.009623205,-0.011314785,-0.080343746,-0.026864782,0.0012804314,-0.030022398,-0.00960441,-0.0075118644,-0.011139362,0.0036149672,-0.0070419814,0.009548024,-0.037490405,0.0848045,0.041550197,-0.019497016,-0.043354545,0.0036901485,-0.114576295,-0.018356767,0.048466872,0.014071432,0.052576784,-0.01280588,-0.018682553,0.003987741,-0.033956885,-0.039821025,0.00028780338,0.015662769,0.008038133,0.01744206,-0.023932712,-0.05049677,0.06590893,-0.021075822,-0.005485102,-0.020186178,0.0067663165,0.043154065,-0.008119579,0.00596125,0.023669578,0.009566819,-0.025298506,0.019459425,-0.0050434116,0.004015934,0.027215628,0.009071876,0.029095162,-0.009403927,0.04688807,-0.003307977,-0.031024814,-0.004175694,-0.022178482,-0.030899512,0.01981027,0.012799615,0.004858591,-0.034733757,-0.025423808,0.04360515,-0.025072962,0.002098811,-0.017417,-0.04114923,-0.002449657,0.012718169,0.004322924,0.0067099305,-0.023331262,0.019070989,0.013419861,-0.01630181,0.029546248,0.010750925,0.0050997976,0.010218391,0.042627793,0.023018006,-0.022867644,-0.008670909,0.010882492,-0.006086552,0.02442139,-0.032202654,0.0052971486,0.005660525,-0.012893592,-0.026914904,0.019284002,0.004122441,0.02325608,0.013156726,-0.011822258,-0.012856001,-0.027391052,0.013394801,0.054431256,-0.03310483,0.028644074,-0.0418008,0.03488412,-0.02388259,-0.016577475,0.040447537,0.033405554,-0.03548557,-0.015387105,-0.017304229,-0.007975482,0.0031184575,-0.009103201,0.026589118,0.013444921,0.04009669,0.038542945,-0.016552415,-0.032102413,-0.031676386,-0.026814662,0.011352375,0.004874254,0.006221252,0.029245524,-0.029120222,-0.018281585,-0.030373244,-0.01452252,0.00551956,0.002087847,-0.013394801,-0.047890484,-0.0061836615,0.03155108,-0.052927632,-0.004858591,0.006186794,-0.022930294,-0.0070607765,0.010024172,-0.0035805092,-0.01977268,0.047389276,-0.015211682,-0.019133639,0.03663835,-0.024571752,0.0058234176,-0.03721474,-0.0058046225,0.01804351,0.012110453,0.002222547,-0.011396231,0.010619358,-0.0028835158,0.028568892,-0.00667234,-0.025348626,0.00788777,0.016477233,-0.03383158,-0.019497016,-0.010913818,0.012549011,0.008282472,-0.0061679985,0.030072518,-0.030398304,0.03142578,0.008188496,-0.0010204294,-0.014184204,-0.01452252,-0.017980859,0.0043197917,0.001458987,0.011270929,-0.0015952531,0.03433279,0.016752899,0.015900843,0.014347097,-0.00053919083,-0.040372357,-0.02383247,0.008796211,-0.01751724,0.04801579,-0.06756292,-0.04410636,0.008031868,0.030523606,0.041525133,0.0191587,-0.02498525,0.02613803,-0.04064802,0.008608258,0.028969858,-0.020699916,0.008789946,-0.009253564,0.007787529,0.010700804,0.032478318,0.013181787,-0.0028177323,0.035886537,0.0015952531,0.019910512,-0.038066797,-0.018832915,-0.018557249,0.00451401,0.0037621972,0.032779045,-0.0038091855,0.017805437,0.024772236,-0.018870505,0.028042622,-0.033455677,-0.029471068,0.008476691,-0.0049901586,-0.049644716,0.026839722,0.036813773,-0.04921869,-0.047890484,-0.05763899,0.00096169405,-0.012242021,-0.019083519,0.017930739,-0.012574071,0.021050762,-0.034783877,-0.03157614,-0.0013000099,-0.0010415742,0.01691579,-0.027741898,0.003687016,0.00594872,-0.022905234,-0.002556164,-0.00034105682,0.037465345,-0.00842657,-0.00037159922,-0.01193503,0.014071432,-0.011277194,-0.0007651263,0.033956885,-0.022353904,-0.006086552,0.00872103,-0.016615067,-0.038618125,0.023945242,0.018269055,-0.010963939,-0.023030536,0.034558333,0.0036995462,-0.058942135,0.027741898,-0.025737062,0.021439198,-0.03380652,0.03721474,0.02269222,0.01073213,-0.0052251,-0.03536027,0.010068028,0.00786271,0.021739924,0.0029320705,-0.009761038,-0.016163979,0.008971634,-0.009366336,0.0013759743,0.0057764295,0.017291697,-0.071923435,0.029095162,0.030097578,0.023694638,-0.019547137,-0.0168782,0.027741898,-0.019559667,-0.021050762,0.005566548,0.00928489,-0.030273002,-0.014635292,-0.028919738,0.023343792,-0.008583197,0.021088352,-0.04413142,-0.027015146,-0.010988999,0.011903705,0.03992127,-0.022880174,-0.0069918605,0.008702234,-0.021451728,0.007192344,-0.00034712613,-0.011415026,-0.0029430345,-0.017830497,-0.0038781017,0.005632332,0.014121553,-0.012110453,-0.0951294,0.031125056,0.0349593,-0.007442948,-0.010569237,0.026363574,-0.011277194,0.05403029,0.00625571,-0.007305116,0.0045891916,-0.018544719,-0.0037183415,-0.020837748,-0.012931183,-0.022253662,-0.019609788,0.054782104,-0.032403138,0.01804351,0.0037246067,0.007267525,0.008639583,0.006691135,-0.031275418,0.045559864,0.011559124,0.009717182,-0.007712348,-0.022792462,0.023895122,0.023494154,0.021677272,-0.019008337,0.023707168,-0.0221033,-0.014322037,0.0023619456,-0.017755315,-0.012555276,0.0076872874,-0.012762025,0.007912831,-0.004936905,-0.009585615,-0.009842484,-0.014760594,-0.021865226,0.028644074,0.016727839,0.04994544,0.015913375,0.02957131,-0.013783237,0.00023083006,0.03270386,0.0006523543,-0.032252774,0.034407973,-0.033355433,-0.008100784,0.03776607,-0.019321593,-0.010443935,-0.0040128017,-0.01276829,0.012467565,0.029270584,-0.030047458,0.007774999,0.0021849563,0.010750925,-0.012893592,-0.018920626,0.010550441,0.011916235,-0.0071735485,0.030298062,0.012824676,-0.035059545,-0.044782992,-0.008019338,-0.020612204,0.03886873,0.0021786913,-0.010882492,-0.007079572,0.010556707,0.0026501406,0.023957772,-0.0011496473,-0.0056573926,0.005306546,-0.009880075,0.014735534,-0.008489221,-0.014560111,-0.03879355,-0.023055596,0.0059737805,0.041449953,-0.011014059,0.029496128,0.0017886882,-0.006312096,-0.025912486,0.016226629,-0.011202013,-0.011740812,-0.029145282,-0.035711113,-0.005525825,0.05633585,0.032503378,0.024183316,0.03769089,-0.013231908,-0.005864141,-0.019885452,0.00066527614,0.012555276,-0.0023180898,-0.034407973,0.013996251,-0.004326057,0.026338514,0.0156753,0.038091857,-0.011928765,-0.025210794,-0.011126831,0.032077353,-0.027365992,-0.0004150634,-0.0019077254,0.00060145033,0.020599674,0.03074915,0.035234965,-0.010594297,0.015662769,-0.012542746,0.0040879827,0.029471068,0.0010783817,0.017066155,-0.061698783,-0.017742785,-0.015286863,-0.01750471,-0.009961521,-0.011289724,-0.03152602,0.00035476172,-0.018807855,-0.03721474,-0.022554388,-0.04190104,0.013156726,0.011214543,0.013908539,0.04543456,-0.0070607765,0.0019844729,-0.010537911,0.015161561,0.05119846,-0.034433033,-0.020073406,-0.023744758,-0.019572197,-0.023168368,0.004598589,0.00046870837,-0.014422278,-0.009529229,0.007956687,0.018294115,-0.0044983476,-0.0011574787,0.003002553,-0.0487676,0.00021027269,-0.0052219676,-0.009009225,-0.022842582,-0.016564945,0.060696363,0.03300459,0.014121553,0.023080658,0.020499432,-0.006193059,0.031225298,0.043905877,-0.011590449,0.034708697,-0.011433822,0.0069292095,0.024258498,0.023155838,-0.028343348,0.052727148,0.02904504,-0.052977752,-0.006615954,-0.0033737605,-0.050145924,-0.02266716,0.055032708,0.00014791529,-0.01745459,-0.009610675,0.0052846186,-0.024245968,-0.034558333,0.016665187,0.033305313,-0.0039125597,-0.01978521,0.0101808,-0.058340684,-0.017304229,0.03024794,-0.016640127,0.0042728037,0.00697933,0.020273888,-0.0030291795,-0.008545607,0.01922135,0.0072737904,0.015424696,0.024596814,0.024521632,0.004119308,0.01867002,0.025686942,-0.014547581,0.014660352,0.03190193,0.002007967,0.004902447,-0.037114497,-0.014647822,0.012116718,0.0028991788,-0.012586602,0.04586059,-0.016076267,-0.007768734,0.038568005,-0.0021301366,0.010945143,0.013595284,0.030874452,-0.0032766515,-0.021188594,-0.013119136,-0.026839722,-0.002449657,0.031250358,-0.0016555547,0.010481525,0.006067757,0.0018168812,0.0038091855,-0.012129249,0.016427113,-0.023068126,0.01102659,0.0025968873,-0.016539885,0.007367767,-0.007724878,0.020812688,-0.012931183,0.012248286,0.00933501,-0.028092744,0.02841853,0.013006364,0.008201026,0.025712002,-0.01742953,0.021714864,-0.0064280005,0.0012373588,0.00048280487,-0.015424696,0.012323467,0.03022288,0.047840364,0.026463816,-0.04074826,0.00096639286,-0.0076872874,0.010306102,0.016502295,-0.018444477,0.036838833,-0.01016827,-0.0064092055,-0.021714864,-0.006152336,0.0032829165,-0.012423709,-0.025712002,-0.0011347677,-0.030398304,-0.018444477,0.015349514,-0.032353017,0.00045304562,-0.0048773866,0.014823245,0.0022804993,0.005632332,-0.0053410046,0.019998224,0.009917665,0.01684061,0.028819496,-0.007868975,0.0038655715,0.0035648465,0.037515465,0.028719254,0.028243106,0.019070989,-0.004654975,0.008107049,-0.028744314,-0.008495486,-0.020136056,0.017028563,-0.021313896,0.016990973,-0.033255193,-0.0049588326,0.00047184093,0.0050590746,-0.0067788465,0.004050392,0.011809728,0.021965468,-0.013670465,0.017667605,0.019371713,0.007994277,0.05874165,0.0049525676,-0.04701337,-0.0076434314,-0.030373244,-0.00507787,0.0031153248,-0.008940309,0.0040817177,-0.0021144738,-0.013294559,-0.00845163,-0.007969217,0.01335721,0.013006364,-0.03528509,-0.0038530412,0.03881861,-0.006860293,-0.0019139905,0.015186622,-0.008984164,0.0024637536,-0.014672883,0.0033800257,-0.013833358,-0.0021708598,-0.032202654,0.017153865,0.0045265406,0.027190568,-0.032954466,0.011659366,0.04425672,-0.015149031,0.013595284,0.0034802675,-0.0033612303,-0.020273888,-0.008414039,0.005933057,0.014447339,0.034558333,-0.02789226,0.009403927,0.009353806,0.03726486,0.01628928,-0.014459869,-0.009936461,-0.019509546,-0.0024386933,0.034834,0.014397218,-0.0060082385,0.018995807,-0.0005920527,0.020424252,0.0022930296,0.019108579,-0.009855014,-0.018181343,0.0067099305,0.036964137,0.0024872478,-0.047539636,0.01511144,0.0031247225,-0.0031623133,0.051474124,0.016439643,-0.002607851,-0.0023650783,0.02497272,0.029395886,-0.008094519,-0.016527355,0.015950965,-0.013169257,-0.019045928,-0.027365992,-0.0076872874,-0.005040279,0.04297864,0.00929742,0.021276306,0.019459425,0.013219378,-0.009635736,-0.044407085,-0.0073427064,-0.005882936,0.026513936,0.00962947,-0.018482069,0.016953383,-0.015274333,0.023155838,-0.019096049,0.029420946,0.033906765,0.0033800257,0.0035272557,0.020825218,0.027215628,-0.022905234,-0.0038499087,0.014710473,0.022416556,0.0038373785,0.0016195304,0.01394613,0.017893149,-0.021201124,0.018294115,0.012448769,0.011985151,-0.030348182,-0.021890286,-0.011540329,-0.0068728235,0.011189482,0.027391052,-0.0168782,-0.02096305,-0.02503537,0.0038248484,-0.008244882,-0.001917123,0.021639682,0.034382913,0.026012728,0.042828277,0.0008222954,0.006904149,0.02271728,0.024496572,0.02784214,0.03430773,0.007035716,-0.06651039,-0.013820828,0.012931183,0.008013073,0.060946967,0.002228812,0.023168368,-0.027541414,0.009654531,0.0081697,0.0045046126,0.009829954,0.00407232,0.00053919083,-0.03613714,-0.0057764295,-0.042001285,-0.027015146,-0.0014903125,-0.013983721,0.008257412,-0.010537911,-0.0077937944,-0.03140072,-0.00538486,-0.0010627189,-0.042778157,-0.013031424,-0.0036462927,0.0010494055,0.024007894,-0.03766583,-0.00081289775,-0.00724873,-0.010619358,0.006803907,-0.0033831582,0.022980416,-0.008038133,0.0012303105,-0.023393912,-0.014560111,0.035260025,0.005729441,-0.001216214,0.02553658,0.044833113,-0.010481525,-0.031099996,0.03318001,0.020023284,-0.031626265,0.022028118,-0.007267525,0.02096305,0.025361156,-0.028393468,0.0013015762,0.01507385,0.00013215462,-0.010318632,0.008257412,-0.0019515811,0.0042383457,-0.017203987,0.038743425,-0.021977998,0.009153322,0.007856445,0.0045547336,-0.003286049,0.006283903,0.023356322,-0.02320596,-0.01627675,-0.0017620616,-0.026789602,0.023682108,0.028644074,0.022516798,0.0156753,-0.049744956,0.03841764,0.03618726,-0.004410636,0.043179125,0.03606196,0.0058798036,0.025348626,0.0033894232,0.032854225,-0.02846865,0.0013211546,0.01914617,-0.00023317948,0.009190913,0.005660525,-0.024559222,-0.005447511,-0.0062964335,-0.013670465,0.00028467085,0.008489221,0.032804105,0.012730699,-0.014823245,0.02214089,-0.01691579,-0.016226629,-0.019020867,0.0026548395,-0.0002725322,-0.0037465345,-0.008188496,-0.008194761,-0.007254995,-0.00053919083,-0.009485373,-0.030097578,0.013632875,-0.003665088,-0.021025702,-0.028844556,-0.00069934264,-0.026438756,-0.001962545,-0.023431504,0.034608457,-0.025373686,0.0019249544,-0.011057915,-0.008984164,-0.00048437115,0.0054819696,-0.027040206,0.004582926,0.005751369,-0.01573795,0.0069668,0.003602437,-0.022404026,-0.0016978442,0.01512397,-0.0060270336,0.010042968,-0.009046815,-0.012035272,0.016978443,0.019559667,0.010725864,-0.003806053,0.024822358,-0.0511734,-0.0022992946,-0.012812146,0.010368753,-0.006355952,-0.022028118,0.026889842,0.022228602,0.007442948,-0.011571654,-0.020562084,-0.0004902447,-0.0017714592,-0.0349593,-0.010851167,0.014334567,0.021802574,-0.02616309,-0.008482956,0.004871121,-0.016978443,0.01636446,0.015925905,-0.013845888,-0.0028177323,0.021990528,0.02098811,0.009792363,-0.017843027,0.025962606,0.021739924,0.0073990924,0.025812244,-0.032252774,-0.02673948,0.0042446107,0.010920083,-0.028919738,-0.018770263,-0.0021536308,0.042051405,-0.029821914,0.01691579,-0.022291254,-0.016176509,0.008846332,-0.041349713,-0.021013172,0.0032453258,0.008213556,0.03726486,-0.015888313,0.01103912,-0.016189039,0.031099996,0.050170984,0.022479206,-0.019584727,-0.007073307,-0.01914617,-0.012417444,-0.01747965,-0.017893149,0.009660796,0.025223324,-0.008846332,0.02440886,0.007154753,-0.022341374,0.010086823,-0.003223398,0.017066155,-0.021075822,0.022880174,0.014898427,-0.010738395,0.015149031,0.027090326,0.032779045,0.010869962,0.020161116,-0.014234325,0.028318288,-0.0049682306,-0.009829954,0.017579893,0.019008337,-0.01508638,0.046286616,0.029947216,0.019509546,0.029897094,0.026488876,0.0052282326,-0.015963495,0.00027018276,0.011252134]\n",
      "node_id: '12':189 '194bn':166 '2':14 '200bn':6,26 '2024':15 '22.5':201 '30bn':294 '41':139 '60bn':128 '62bn':220 '65':137 '70bn':285 'abil':83 'accord':175,230 'acquisit':78,203 'acreag':58,370 'across':170 'advis':123 'afield':396 'ahead':90 'alfaro':253 'almost':25,182,400 'also':266 'amount':185 'analyst':323 'announc':165,199 'anoth':219 'asset':222 'attract':279 'bank':120 'base':281,289 'basin':377 'beaten':334 'becom':311 'best':56,369 'beyond':70 'bigger':249 'biggest':33,297 'bn':202 'bolster':81 'boutiqu':118 'bulk':321 'burst':359 'came':194 'cast':64 'chevron':160 'chief':112 'chord':239 'civita':242 'comment':354 'compani':62,145,157,211,233,271,326,392 'compet':35 'conoco':339 'conocophillip':198 'consolid':98,397 'consult':177 'countri':54,388 'date':12 'deal':169 'deal.eog':345 'dealmak':7,16,358 'devon':290,309,347 'doubl':273 'drill':57 'eagl':274 'energi':10,50,179,240,264,291 'engin':384 'enter':361 'eog':282 'execut':113 'exxon':159 'exxonmobil':131 'fail':319 'familiar':342 'field':406 'financi':215 'firm':121 'five':152 'focus':260 'fund':259 'gallo':255 'gas':21,144 'gone':134 'hedg':258 'held':270,328 'hugh':111 'hydrocarbon':86 'includ':158,234,272 'industri':4,262,391 'invest':119 'jon':110 'juli':156 'june':13 'known':224 'landscap':11 'landscape.but':51 'last':155 'latest':193 'least':218 'left':399 'less':150 'list':299 'look':69,394 'marathon':205,331 'market':229 'matador':237 'mewbourn':276 'mexico':382 'michael':252 'midst':95 'month':190 'much':366 'nation':49 'natur':126 'net':67 'new':363,381 'occident':162 'oil':19,142,206,277,390,409 'oilfield':76 'oklahoma':288 'oklahoma-bas':287 'partner':116,256 'past':29 'patch':174 'peopl':341 'period':191 'permian':235,376 'petri':115 'petroleum':163 'phase':364 'pioneer':125 'plans.the':357 'player':250,301,316 'point':267 'previous':188 'privat':269 'produc':34 'prolif':375 'prospects.houston':280 'public':140,298 'pump':85 'punch':337 'race':42 'recent':307 'redraw':9 'redrawn':47 'report':213 'request':352 'resourc':127,236,238,243 'respond':350 'risk':310 'rival':39 'room':385 'rystad':178,232 'said':109,251,324,340 'sale':129 'scale':44 'shale':3,173,408 'sight':247 'sinc':154 'snap':60 'sought':74 'sought-aft':73 'spoken':371 'strike':304 'surg':23 'swallow':37 'talk':208,329 'target':313 'texa':379 'think':104 'third':403 'time':216 'titl':1 'trade':141 'tripl':183 'two':402 'two-third':401 'us':2,18,148,172,300 'valu':283,292 've':133 'wave':8,99,308 'week':196 'wider':66 'worth':167 'year':30,89,153 'yet':108,302\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as c:\n",
    "    # Replace 'data_llama2_paper' with your table name\n",
    "    table_name = \"data_llama2_paper\"\n",
    "    \n",
    "    # Retrieve one or a few rows from the table\n",
    "    c.execute(f\"SELECT * FROM {table_name} LIMIT 1;\")\n",
    "    sample_node = c.fetchone()\n",
    "    \n",
    "    # Get the column names\n",
    "    c.execute(f\"SELECT column_name FROM information_schema.columns WHERE table_name = '{table_name}';\")\n",
    "    columns = [col[0] for col in c.fetchall()]\n",
    "    \n",
    "    # Print the column names and their corresponding values in the sample node\n",
    "    print(f\"Sample node from {table_name}:\")\n",
    "    for col_name, value in zip(columns, sample_node):\n",
    "        print(f\"{col_name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# energy\n",
    "\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from fpdf import FPDF\n",
    "import uuid\n",
    "import unidecode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "\n",
    "file_name = 'news_data_energy.csv' # change to proper data\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "path = './data3/'\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"o-topper__headline o-topper__headline--large\", \"n-content-body js-article__content-body\"))\n",
    "all_nodes = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    heading = row['Headline']\n",
    "    href_tag = row['Link']\n",
    "    date = row['Date']\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(href_tag, 'html.parser')\n",
    "    href = soup.find('a')['href']\n",
    "    \n",
    "    myurl = 'https://www.ft.com' + href\n",
    "    \n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(myurl,),\n",
    "        bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    content = docs[0].page_content\n",
    "    \n",
    "    heading1 = unidecode.unidecode(heading)\n",
    "    date1 = unidecode.unidecode(date)\n",
    "    content = unidecode.unidecode(content)\n",
    "\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.cell(200, 10, txt=\"Title: \" + heading1, ln=True)\n",
    "    pdf.cell(200, 10, txt=\"Date: \" + date1, ln=True)\n",
    "    pdf.multi_cell(0, 10, txt=content)\n",
    "\n",
    "    file_name = \"temp.pdf\" # overwriting the files to optimise space complexity\n",
    "    full_path = path + file_name\n",
    "\n",
    "    pdf.output(full_path) # the pdf ready to be passed to the vdb\n",
    "    \n",
    "    # -------------------------- up to here the current pdf has just been created and is in ./data3/temp.pdf\n",
    "    \n",
    "    loaderv = PyMuPDFReader()\n",
    "    documents = loaderv.load(file_path=\"./data3/temp.pdf\")\n",
    "    \n",
    "    text_parser = SentenceSplitter(\n",
    "        chunk_size=1024,\n",
    "    )\n",
    "    \n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "    \n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(\n",
    "            text=text_chunk,\n",
    "        )\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        node.metadata[\"date\"] = date1\n",
    "        nodes.append(node)\n",
    "    \n",
    "    for node in nodes:\n",
    "        node_embedding = get_openai_embedding(node.get_content(metadata_mode=\"all\"))\n",
    "        node.embedding = node_embedding\n",
    "        \n",
    "    all_nodes.extend(nodes)\n",
    "    hybrid_vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.ft.comhttps', port=443): Max retries exceeded with url: /www.pwmnet.com/investing-in-multipolar-portfolios (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x13d7663c0>: Failed to resolve 'www.ft.comhttps' ([Errno 8] nodename nor servname provided, or not known)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:196\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/socket.py:963\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    962\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    964\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:615\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x13d7663c0>: Failed to resolve 'www.ft.comhttps' ([Errno 8] nodename nor servname provided, or not known)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.ft.comhttps', port=443): Max retries exceeded with url: /www.pwmnet.com/investing-in-multipolar-portfolios (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x13d7663c0>: Failed to resolve 'www.ft.comhttps' ([Errno 8] nodename nor servname provided, or not known)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m myurl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.ft.com\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m href\n\u001b[1;32m     21\u001b[0m loader \u001b[38;5;241m=\u001b[39m WebBaseLoader(\n\u001b[1;32m     22\u001b[0m     web_paths\u001b[38;5;241m=\u001b[39m(myurl,),\n\u001b[1;32m     23\u001b[0m     bs_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparse_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: bs4_strainer},\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m content \u001b[38;5;241m=\u001b[39m docs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content\n\u001b[1;32m     29\u001b[0m heading1 \u001b[38;5;241m=\u001b[39m unidecode\u001b[38;5;241m.\u001b[39munidecode(heading)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_core/document_loaders/base.py:29\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_community/document_loaders/web_base.py:253\u001b[0m, in \u001b[0;36mWebBaseLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Lazy load text from the url(s) in web_path.\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweb_paths:\n\u001b[0;32m--> 253\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     text \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mget_text(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbs_get_text_kwargs)\n\u001b[1;32m    255\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m _build_metadata(soup, path)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_community/document_loaders/web_base.py:232\u001b[0m, in \u001b[0;36mWebBaseLoader._scrape\u001b[0;34m(self, url, parser, bs_kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m         parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_parser\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_parser(parser)\n\u001b[0;32m--> 232\u001b[0m html_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequests_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraise_for_status:\n\u001b[1;32m    234\u001b[0m     html_doc\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/adapters.py:622\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.ft.comhttps', port=443): Max retries exceeded with url: /www.pwmnet.com/investing-in-multipolar-portfolios (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x13d7663c0>: Failed to resolve 'www.ft.comhttps' ([Errno 8] nodename nor servname provided, or not known)\"))"
     ]
    }
   ],
   "source": [
    "# fin\n",
    "\n",
    "file_name = 'news_data_fin.csv' # change to proper data\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "path = './data3/'\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"o-topper__headline o-topper__headline--large\", \"n-content-body js-article__content-body\"))\n",
    "all_nodes_fin = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    heading = row['Headline']\n",
    "    href_tag = row['Link']\n",
    "    date = row['Date']\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(href_tag, 'html.parser')\n",
    "    href = soup.find('a')['href']\n",
    "    \n",
    "    myurl = 'https://www.ft.com' + href\n",
    "    \n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(myurl,),\n",
    "        bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    content = docs[0].page_content\n",
    "    \n",
    "    heading1 = unidecode.unidecode(heading)\n",
    "    date1 = unidecode.unidecode(date)\n",
    "    content = unidecode.unidecode(content)\n",
    "\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.cell(200, 10, txt=\"Title: \" + heading1, ln=True)\n",
    "    pdf.cell(200, 10, txt=\"Date: \" + date1, ln=True)\n",
    "    pdf.multi_cell(0, 10, txt=content)\n",
    "\n",
    "    file_name = \"temp.pdf\" # overwriting the files to optimise space complexity\n",
    "    full_path = path + file_name\n",
    "\n",
    "    pdf.output(full_path) # the pdf ready to be passed to the vdb\n",
    "    \n",
    "    # -------------------------- up to here the current pdf has just been created and is in ./data3/temp.pdf\n",
    "    \n",
    "    loaderv = PyMuPDFReader()\n",
    "    documents = loaderv.load(file_path=\"./data3/temp.pdf\")\n",
    "    \n",
    "    text_parser = SentenceSplitter(\n",
    "        chunk_size=1024,\n",
    "    )\n",
    "    \n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "    \n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(\n",
    "            text=text_chunk,\n",
    "        )\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        node.metadata[\"date\"] = date1\n",
    "        nodes.append(node)\n",
    "    \n",
    "    for node in nodes:\n",
    "        node_embedding = get_openai_embedding(node.get_content(metadata_mode=\"all\"))\n",
    "        node.embedding = node_embedding\n",
    "        \n",
    "    all_nodes_fin.extend(nodes)\n",
    "    hybrid_vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fin 2 bc previous crashes at a wrong url\n",
    "\n",
    "file_name = 'news_data_fin2.csv' # change to proper data\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "path = './data3/'\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"o-topper__headline o-topper__headline--large\", \"n-content-body js-article__content-body\"))\n",
    "all_nodes_fin2 = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    heading = row['Headline']\n",
    "    href_tag = row['Link']\n",
    "    date = row['Date']\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(href_tag, 'html.parser')\n",
    "    href = soup.find('a')['href']\n",
    "    \n",
    "    myurl = 'https://www.ft.com' + href\n",
    "    \n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(myurl,),\n",
    "        bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    content = docs[0].page_content\n",
    "    \n",
    "    heading1 = unidecode.unidecode(heading)\n",
    "    date1 = unidecode.unidecode(date)\n",
    "    content = unidecode.unidecode(content)\n",
    "\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.cell(200, 10, txt=\"Title: \" + heading1, ln=True)\n",
    "    pdf.cell(200, 10, txt=\"Date: \" + date1, ln=True)\n",
    "    pdf.multi_cell(0, 10, txt=content)\n",
    "\n",
    "    file_name = \"temp.pdf\" # overwriting the files to optimise space complexity\n",
    "    full_path = path + file_name\n",
    "\n",
    "    pdf.output(full_path) # the pdf ready to be passed to the vdb\n",
    "    \n",
    "    # -------------------------- up to here the current pdf has just been created and is in ./data3/temp.pdf\n",
    "    \n",
    "    loaderv = PyMuPDFReader()\n",
    "    documents = loaderv.load(file_path=\"./data3/temp.pdf\")\n",
    "    \n",
    "    text_parser = SentenceSplitter(\n",
    "        chunk_size=1024,\n",
    "    )\n",
    "    \n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "    \n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(\n",
    "            text=text_chunk,\n",
    "        )\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        node.metadata[\"date\"] = date1\n",
    "        nodes.append(node)\n",
    "    \n",
    "    for node in nodes:\n",
    "        node_embedding = get_openai_embedding(node.get_content(metadata_mode=\"all\"))\n",
    "        node.embedding = node_embedding\n",
    "        \n",
    "    all_nodes_fin2.extend(nodes)\n",
    "    hybrid_vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2083\n",
      "1582\n"
     ]
    }
   ],
   "source": [
    "print(len(all_nodes_fin))\n",
    "print(len(all_nodes_fin2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tech\n",
    "\n",
    "file_name = 'news_data_tech.csv' # change to proper data\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "path = './data3/'\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"o-topper__headline o-topper__headline--large\", \"n-content-body js-article__content-body\"))\n",
    "all_nodes_tech = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    heading = row['Headline']\n",
    "    href_tag = row['Link']\n",
    "    date = row['Date']\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(href_tag, 'html.parser')\n",
    "    href = soup.find('a')['href']\n",
    "    \n",
    "    myurl = 'https://www.ft.com' + href\n",
    "    \n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(myurl,),\n",
    "        bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    content = docs[0].page_content\n",
    "    \n",
    "    heading1 = unidecode.unidecode(heading)\n",
    "    date1 = unidecode.unidecode(date)\n",
    "    content = unidecode.unidecode(content)\n",
    "\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.cell(200, 10, txt=\"Title: \" + heading1, ln=True)\n",
    "    pdf.cell(200, 10, txt=\"Date: \" + date1, ln=True)\n",
    "    pdf.multi_cell(0, 10, txt=content)\n",
    "\n",
    "    file_name = \"temp.pdf\" # overwriting the files to optimise space complexity\n",
    "    full_path = path + file_name\n",
    "\n",
    "    pdf.output(full_path) # the pdf ready to be passed to the vdb\n",
    "    \n",
    "    # -------------------------- up to here the current pdf has just been created and is in ./data3/temp.pdf\n",
    "    \n",
    "    loaderv = PyMuPDFReader()\n",
    "    documents = loaderv.load(file_path=\"./data3/temp.pdf\")\n",
    "    \n",
    "    text_parser = SentenceSplitter(\n",
    "        chunk_size=1024,\n",
    "    )\n",
    "    \n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "    \n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(\n",
    "            text=text_chunk,\n",
    "        )\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        node.metadata[\"date\"] = date1\n",
    "        nodes.append(node)\n",
    "    \n",
    "    for node in nodes:\n",
    "        node_embedding = get_openai_embedding(node.get_content(metadata_mode=\"all\"))\n",
    "        node.embedding = node_embedding\n",
    "        \n",
    "    all_nodes_tech.extend(nodes)\n",
    "    hybrid_vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tech\n",
    "\n",
    "file_name = 'news_data_tech2.csv' # change to proper data\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "path = './data3/'\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"o-topper__headline o-topper__headline--large\", \"n-content-body js-article__content-body\"))\n",
    "all_nodes_tech2 = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    heading = row['Headline']\n",
    "    href_tag = row['Link']\n",
    "    date = row['Date']\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(href_tag, 'html.parser')\n",
    "    href = soup.find('a')['href']\n",
    "    \n",
    "    myurl = 'https://www.ft.com' + href\n",
    "    \n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(myurl,),\n",
    "        bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    content = docs[0].page_content\n",
    "    \n",
    "    heading1 = unidecode.unidecode(heading)\n",
    "    date1 = unidecode.unidecode(date)\n",
    "    content = unidecode.unidecode(content)\n",
    "\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.cell(200, 10, txt=\"Title: \" + heading1, ln=True)\n",
    "    pdf.cell(200, 10, txt=\"Date: \" + date1, ln=True)\n",
    "    pdf.multi_cell(0, 10, txt=content)\n",
    "\n",
    "    file_name = \"temp.pdf\" # overwriting the files to optimise space complexity\n",
    "    full_path = path + file_name\n",
    "\n",
    "    pdf.output(full_path) # the pdf ready to be passed to the vdb\n",
    "    \n",
    "    # -------------------------- up to here the current pdf has just been created and is in ./data3/temp.pdf\n",
    "    \n",
    "    loaderv = PyMuPDFReader()\n",
    "    documents = loaderv.load(file_path=\"./data3/temp.pdf\")\n",
    "    \n",
    "    text_parser = SentenceSplitter(\n",
    "        chunk_size=1024,\n",
    "    )\n",
    "    \n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "    \n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(\n",
    "            text=text_chunk,\n",
    "        )\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        node.metadata[\"date\"] = date1\n",
    "        nodes.append(node)\n",
    "    \n",
    "    for node in nodes:\n",
    "        node_embedding = get_openai_embedding(node.get_content(metadata_mode=\"all\"))\n",
    "        node.embedding = node_embedding\n",
    "        \n",
    "    all_nodes_tech2.extend(nodes)\n",
    "    hybrid_vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Date: September 15, 2023\n",
      "Maximum Date: July 29, 2024\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "dates = [datetime.strptime(node.metadata[\"date\"], \"%B %d, %Y\") for node in all_nodes]  # Adjusted date format\n",
    "\n",
    "# Find the minimum and maximum dates\n",
    "min_date = min(dates)\n",
    "max_date = max(dates)\n",
    "\n",
    "print(f\"Minimum Date: {min_date.strftime('%B %d, %Y')}\")\n",
    "print(f\"Maximum Date: {max_date.strftime('%B %d, %Y')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2863\n",
      "2368\n",
      "2613\n"
     ]
    }
   ],
   "source": [
    "# print(len(all_nodes))\n",
    "# print(len(all_nodes_tech))\n",
    "# print(len(all_nodes_tech2))\n",
    "\n",
    "# 2863 2368 2613 2083 1582, 12k nodes ish\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RETRIEVAL - dont rerun previous cells now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "query_str = \"Who bought Marathon Oil?\" # ConocoPhilips\n",
    "query_embedding = get_openai_embedding(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vector store query\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "query_mode = \"default\" # sparse or hybrid\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = hybrid_vector_store.query(vector_store_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_result.nodes):\n",
    "    score: Optional[float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "    nodes_with_scores.append(NodeWithScore(node=node, score=score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple's stock price has increased by 24% since May 1, 2024, adding over $600 billion in market value. This rise in value positions Apple as one of the top performers, with only Nvidia surpassing it in terms of value growth. Analysts' expectations for Apple's financial results have mainly increased for the years 2026 and 2027, with revenue and earnings growth expectations rising significantly for those years. Despite the positive outlook for Apple's future performance, there has been a notable increase in the company's valuation premium compared to the S&P 500 index, indicating high expectations from investors.\n",
      "Apple's stock price has increased by 24% since May 1, 2024, adding over $600 billion in market value. This surge has positioned Apple as one of the top performers in terms of market value growth, with only Nvidia surpassing it. The rise in Apple's stock price seems to be linked to its perceived transition into an AI stock, as expectations for the company's financial results have grown significantly for the years 2026 and 2027. Despite the surge in analysts' expectations for future revenue and earnings growth, there has also been a notable increase in Apple's valuation premium compared to the S&P 500 index, indicating high expectations and investor confidence in the company's future prospects.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = get_openai_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = hybrid_vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores\n",
    "\n",
    "retriever_old = VectorDBRetriever(\n",
    "    hybrid_vector_store, embed_model.get_embedding, query_mode=\"default\", similarity_top_k=2\n",
    ")\n",
    "\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "query_engine_old = RetrieverQueryEngine(\n",
    "    retriever=retriever_old,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    # node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "\n",
    "### HERE, IS BETTER, there is clear data leakage though\n",
    "\n",
    "response1 = query_engine_old.query(\"Provide a summary of stock price change relevant information of Apple (AAPL) up to December 2023.\")\n",
    "\n",
    "# response2 = query_engine_old.query(\"Provide a summary of stock price change relevant information of Apple (AAPL) up to July 2024.\")\n",
    "# response = query_engine_old.query(\"whats your prediction for stock price movement of conocophilips based on the info you have on them and its industry? up or down?\")\n",
    "\n",
    "print(response1)\n",
    "# print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter Date: June 1, 2024, Parsed Filter Date: 2024-06-01 00:00:00\n",
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List, Optional\n",
    "from datetime import datetime \n",
    "\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "    except ValueError:\n",
    "        return None  # Handle error appropriately\n",
    "        \n",
    "class VectorDBRetriever2(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "        \n",
    "    def _retrieve(self, query_bundle: QueryBundle, filter_date: Optional[str] = None) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = get_openai_embedding(query_bundle.query_str)\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = hybrid_vector_store.query(vector_store_query)\n",
    "        \n",
    "        filter_datetime = parse_date(filter_date) if filter_date else None\n",
    "        print(f\"Filter Date: {filter_date}, Parsed Filter Date: {filter_datetime}\")\n",
    "\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            node_date_str = node.metadata.get(\"date\")\n",
    "            node_date = parse_date(node_date_str) if node_date_str else None\n",
    "\n",
    "            if node_date and (not filter_datetime or node_date < filter_datetime):\n",
    "                score: Optional[float] = None\n",
    "                if query_result.similarities is not None:\n",
    "                    score = query_result.similarities[index]\n",
    "                nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores\n",
    "\n",
    "retriever = VectorDBRetriever2(\n",
    "    hybrid_vector_store, embed_model.get_embedding, query_mode=\"default\", similarity_top_k=2\n",
    ")\n",
    "\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "from llama_index.core.callbacks.schema import CBEventType, EventPayload\n",
    "from llama_index.core.base.response.schema import RESPONSE_TYPE\n",
    "\n",
    "\n",
    "\n",
    "class CRetrieverQueryEngine(RetrieverQueryEngine):\n",
    "    # Other parts of the class remain unchanged\n",
    "    def __init__(self, retriever, response_synthesizer, node_postprocessors=None):\n",
    "        super().__init__(retriever=retriever, response_synthesizer=response_synthesizer, node_postprocessors=node_postprocessors)\n",
    "        \n",
    "    def _query(self, query_bundle: QueryBundle, filter_date: Optional[str] = None) -> RESPONSE_TYPE:\n",
    "        \"\"\"Answer a query.\"\"\"\n",
    "        with self.callback_manager.event(\n",
    "            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n",
    "        ) as query_event:\n",
    "            nodes = self._retriever._retrieve(query_bundle, filter_date)\n",
    "            response = self._response_synthesizer.synthesize(\n",
    "                query=query_bundle,\n",
    "                nodes=nodes,\n",
    "            )\n",
    "            query_event.on_end(payload={EventPayload.RESPONSE: response})\n",
    "\n",
    "        return response\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "query_engine = CRetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "query_bundle = QueryBundle(query_str=\"Give a summary of information relevant to the stock price trend of Apple (AAPL). Consider the industry and global events that may influence it. Today is July 1st 2024, and I will want to predict the stock price movement for tomorrow, so consider the time when reporting the relevance of the news.\")\n",
    "\n",
    "response = query_engine._query(query_bundle, filter_date=\"June 1, 2024\")\n",
    "print(response)\n",
    "\n",
    "# query_embedding = get_openai_embedding(query_bundle.query_str)\n",
    "# print(f\"Embedding for '{query_bundle.query_str}': {query_embedding}\")\n",
    "\n",
    "# query_result = hybrid_vector_store.query(vector_store_query)\n",
    "# print(f\"Query result nodes count: {len(query_result.nodes)}\")\n",
    "\n",
    "# print(f\"Synthesized response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering nodes before: 2024-06-01 00:00:00\n",
      "Excluded Node with Date: 2024-07-01 00:00:00\n",
      "Excluded Node with Date: 2024-07-01 00:00:00\n",
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "# aug 30\n",
    "\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper function to parse date strings\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "    except ValueError:\n",
    "        print(f\"Error parsing date: {date_str}\")\n",
    "        return None  # Handle error appropriately\n",
    "\n",
    "class VectorDBRetriever2(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store with date filtering.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 10,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle, filter_date: Optional[str] = None) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve and then filter based on date.\"\"\"\n",
    "        query_embedding = get_openai_embedding(query_bundle.query_str)\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = hybrid_vector_store.query(vector_store_query)\n",
    "        \n",
    "        filter_datetime = parse_date(filter_date) if filter_date else None\n",
    "        if filter_datetime:\n",
    "            print(f\"Filtering nodes before: {filter_datetime}\")\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            node_date_str = node.metadata.get(\"date\")\n",
    "            node_date = parse_date(node_date_str) if node_date_str else None\n",
    "\n",
    "            # Include nodes that match the date filter\n",
    "            if not filter_datetime or (node_date and node_date < filter_datetime):\n",
    "                score: Optional[float] = None\n",
    "                if query_result.similarities is not None:\n",
    "                    score = query_result.similarities[index]\n",
    "                nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "            else:\n",
    "                print(f\"Excluded Node with Date: {node_date}\")\n",
    "\n",
    "        return nodes_with_scores\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = VectorDBRetriever2(\n",
    "    hybrid_vector_store, embed_model.get_embedding, query_mode=\"default\", similarity_top_k=2\n",
    ")\n",
    "\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# Custom query engine to use our retriever with filtering\n",
    "class CRetrieverQueryEngine(RetrieverQueryEngine):\n",
    "    def __init__(self, retriever, response_synthesizer, node_postprocessors=None):\n",
    "        super().__init__(retriever=retriever, response_synthesizer=response_synthesizer, node_postprocessors=node_postprocessors)\n",
    "        \n",
    "    def _query(self, query_bundle: QueryBundle, filter_date: Optional[str] = None) -> RESPONSE_TYPE:\n",
    "        \"\"\"Answer a query with date filtering.\"\"\"\n",
    "        with self.callback_manager.event(\n",
    "            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n",
    "        ) as query_event:\n",
    "            nodes = self._retriever._retrieve(query_bundle, filter_date)\n",
    "            response = self._response_synthesizer.synthesize(\n",
    "                query=query_bundle,\n",
    "                nodes=nodes,\n",
    "            )\n",
    "            query_event.on_end(payload={EventPayload.RESPONSE: response})\n",
    "\n",
    "        return response\n",
    "\n",
    "# Initialize the response synthesizer and query engine\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "query_engine = CRetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# Example usage with a specified filter date\n",
    "query_bundle = QueryBundle(query_str=\"Provide a summary of stock price change relevant information of Apple (AAPL) up to June 2024.\")\n",
    "\n",
    "# Use filter_date to only include nodes before June 1, 2024\n",
    "response = query_engine._query(query_bundle, filter_date=\"June 1, 2024\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 30.00%\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "\n",
    "data = []\n",
    "with open('./data/prices/csvs/AAPL.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip header\n",
    "    for row in reader:\n",
    "        data.append(float(row[1]))\n",
    "        \n",
    "\n",
    "data.reverse() #from oldest to most recent\n",
    "\n",
    "sequences = [data[i:i+5] for i in range(10)] #len(data) - 5)] only 10 for now\n",
    "actual_trends = ['up' if data[i+5] > data[i+4] else 'down' for i in range(len(data) - 5)]\n",
    "\n",
    "def get_model_prediction(prices):\n",
    "    \n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Predict the stock trend based on these prices: {prices}. Your answer should either be 'up' or 'down', do not elaborate.\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content #completion.choices[0].text.strip()\n",
    "\n",
    "accurate_values = 0\n",
    "for prices, true_trend in zip(sequences[:10], actual_trends[:10]):  # Limit to 10 calls as per your example\n",
    "    predicted_trend = get_model_prediction(prices)\n",
    "    # print(predicted_trend)\n",
    "    if predicted_trend.lower() == true_trend:\n",
    "        accurate_values += 1\n",
    "\n",
    "print(f\"Accuracy: {accurate_values/10:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector_stores\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     MetadataFilter,\n\u001b[1;32m      7\u001b[0m     MetadataFilters,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m index \u001b[38;5;241m=\u001b[39m VectorStoreIndex\u001b[38;5;241m.\u001b[39mfrom_vector_store(vector_store\u001b[38;5;241m=\u001b[39mhybrid_vector_store)\n\u001b[0;32m---> 10\u001b[0m index\u001b[38;5;241m.\u001b[39minsert_nodes(\u001b[43mall_nodes\u001b[49m)\n\u001b[1;32m     11\u001b[0m index\u001b[38;5;241m.\u001b[39minsert_nodes(all_nodes_fin)\n\u001b[1;32m     12\u001b[0m index\u001b[38;5;241m.\u001b[39minsert_nodes(all_nodes_fin2)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_nodes' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    ")\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=hybrid_vector_store)\n",
    "index.insert_nodes(all_nodes)\n",
    "index.insert_nodes(all_nodes_fin)\n",
    "index.insert_nodes(all_nodes_fin2)\n",
    "index.insert_nodes(all_nodes_tech)\n",
    "index.insert_nodes(all_nodes_tech2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_pages': 2, 'file_path': './data3/temp.pdf', 'source': '2', 'date': 'July 11, 2024'}\n",
      "{'total_pages': 2, 'file_path': './data3/temp.pdf', 'source': '1', 'date': 'March 4, 2024'}\n",
      "No, there is no information provided in the context about ConocoPhilips buying Marathon Oil.\n"
     ]
    }
   ],
   "source": [
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=\"date\", value=\"June 4, 2024\", operator=\"<=\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=10,\n",
    "    # filters=filters,\n",
    ")\n",
    "\n",
    "retrieved_nodes = retriever.retrieve(\"Did ConocoPhilips buy Marathon Oil?\")\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    print(node.node.metadata)\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    # node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"Did ConocoPhilips buy Marathon Oil?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
