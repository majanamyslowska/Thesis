{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE A VECTOR, HYBRID DB based on the csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "import textwrap\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key='sk-e46gtJZjdqqgxbe9i9tPbMwRxEbuH3bcSd6lTJa9TMT3BlbkFJFXbBGoSo9k4ehTjaCF7l-Vl0wj4jBf1LpvkKN8E1sA',\n",
    ")\n",
    "\n",
    "class OpenAIEmbeddingModel:\n",
    "    def __init__(self, model_name=\"text-embedding-3-small\"):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        return client.embeddings.create(input = [text], model=self.model_name).data[0].embedding\n",
    "    \n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "embed_model = OpenAIEmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL version: ('PostgreSQL 14.12 (Homebrew) on x86_64-apple-darwin23.4.0, compiled by Apple clang version 15.0.0 (clang-1500.3.9.4), 64-bit',)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "db_name = \"vdb\" # change the db name here\n",
    "host = \"localhost\"\n",
    "password = \"password\"\n",
    "port = \"5432\"\n",
    "user = \"maja2\"\n",
    "\n",
    "# conn = psycopg2.connect(\n",
    "#     dbname=\"postgres\",\n",
    "#     host=host,\n",
    "#     password=password,\n",
    "#     port=port,\n",
    "#     user=user,\n",
    "# )\n",
    "# conn.autocommit = True\n",
    "\n",
    "# with conn.cursor() as c:\n",
    "#     c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "#     c.execute(f\"CREATE DATABASE {db_name}\")\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    c.execute(\"SELECT version();\")  # Get the version of the PostgreSQL server\n",
    "    version = c.fetchone()\n",
    "    print(\"Connected to PostgreSQL version:\", version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "\n",
    "connection_string = \"postgresql://postgres:password@localhost:5432\"\n",
    "\n",
    "hybrid_vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"llama2_paper\",\n",
    "    embed_dim=1536,\n",
    "    hybrid_search=True,\n",
    "    text_search_config=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# energy\n",
    "\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from fpdf import FPDF\n",
    "import uuid\n",
    "import unidecode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "\n",
    "file_name = 'news_data_energy.csv' # change to proper data\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "path = './data3/'\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"o-topper__headline o-topper__headline--large\", \"n-content-body js-article__content-body\"))\n",
    "all_nodes = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    heading = row['Headline']\n",
    "    href_tag = row['Link']\n",
    "    date = row['Date']\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(href_tag, 'html.parser')\n",
    "    href = soup.find('a')['href']\n",
    "    \n",
    "    myurl = 'https://www.ft.com' + href\n",
    "    \n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(myurl,),\n",
    "        bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    content = docs[0].page_content\n",
    "    \n",
    "    heading1 = unidecode.unidecode(heading)\n",
    "    date1 = unidecode.unidecode(date)\n",
    "    content = unidecode.unidecode(content)\n",
    "\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.cell(200, 10, txt=\"Title: \" + heading1, ln=True)\n",
    "    pdf.cell(200, 10, txt=\"Date: \" + date1, ln=True)\n",
    "    pdf.multi_cell(0, 10, txt=content)\n",
    "\n",
    "    file_name = \"temp.pdf\" # overwriting the files to optimise space complexity\n",
    "    full_path = path + file_name\n",
    "\n",
    "    pdf.output(full_path) # the pdf ready to be passed to the vdb\n",
    "    \n",
    "    # -------------------------- up to here the current pdf has just been created and is in ./data3/temp.pdf\n",
    "    \n",
    "    loaderv = PyMuPDFReader()\n",
    "    documents = loaderv.load(file_path=\"./data3/temp.pdf\")\n",
    "    \n",
    "    text_parser = SentenceSplitter(\n",
    "        chunk_size=1024,\n",
    "    )\n",
    "    \n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "    \n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(\n",
    "            text=text_chunk,\n",
    "        )\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        node.metadata[\"date\"] = date1\n",
    "        nodes.append(node)\n",
    "    \n",
    "    for node in nodes:\n",
    "        node_embedding = get_openai_embedding(node.get_content(metadata_mode=\"all\"))\n",
    "        node.embedding = node_embedding\n",
    "        \n",
    "    all_nodes.extend(nodes)\n",
    "    hybrid_vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.ft.comhttps', port=443): Max retries exceeded with url: /www.pwmnet.com/investing-in-multipolar-portfolios (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x13d7663c0>: Failed to resolve 'www.ft.comhttps' ([Errno 8] nodename nor servname provided, or not known)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:196\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/socket.py:963\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    962\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    964\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:615\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x13d7663c0>: Failed to resolve 'www.ft.comhttps' ([Errno 8] nodename nor servname provided, or not known)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.ft.comhttps', port=443): Max retries exceeded with url: /www.pwmnet.com/investing-in-multipolar-portfolios (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x13d7663c0>: Failed to resolve 'www.ft.comhttps' ([Errno 8] nodename nor servname provided, or not known)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m myurl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.ft.com\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m href\n\u001b[1;32m     21\u001b[0m loader \u001b[38;5;241m=\u001b[39m WebBaseLoader(\n\u001b[1;32m     22\u001b[0m     web_paths\u001b[38;5;241m=\u001b[39m(myurl,),\n\u001b[1;32m     23\u001b[0m     bs_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparse_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: bs4_strainer},\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m content \u001b[38;5;241m=\u001b[39m docs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content\n\u001b[1;32m     29\u001b[0m heading1 \u001b[38;5;241m=\u001b[39m unidecode\u001b[38;5;241m.\u001b[39munidecode(heading)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_core/document_loaders/base.py:29\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_community/document_loaders/web_base.py:253\u001b[0m, in \u001b[0;36mWebBaseLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Lazy load text from the url(s) in web_path.\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweb_paths:\n\u001b[0;32m--> 253\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     text \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mget_text(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbs_get_text_kwargs)\n\u001b[1;32m    255\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m _build_metadata(soup, path)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_community/document_loaders/web_base.py:232\u001b[0m, in \u001b[0;36mWebBaseLoader._scrape\u001b[0;34m(self, url, parser, bs_kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m         parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_parser\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_parser(parser)\n\u001b[0;32m--> 232\u001b[0m html_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequests_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraise_for_status:\n\u001b[1;32m    234\u001b[0m     html_doc\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/adapters.py:622\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.ft.comhttps', port=443): Max retries exceeded with url: /www.pwmnet.com/investing-in-multipolar-portfolios (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x13d7663c0>: Failed to resolve 'www.ft.comhttps' ([Errno 8] nodename nor servname provided, or not known)\"))"
     ]
    }
   ],
   "source": [
    "# fin\n",
    "\n",
    "file_name = 'news_data_fin.csv' # change to proper data\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "path = './data3/'\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"o-topper__headline o-topper__headline--large\", \"n-content-body js-article__content-body\"))\n",
    "all_nodes_fin = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    heading = row['Headline']\n",
    "    href_tag = row['Link']\n",
    "    date = row['Date']\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(href_tag, 'html.parser')\n",
    "    href = soup.find('a')['href']\n",
    "    \n",
    "    myurl = 'https://www.ft.com' + href\n",
    "    \n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(myurl,),\n",
    "        bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    content = docs[0].page_content\n",
    "    \n",
    "    heading1 = unidecode.unidecode(heading)\n",
    "    date1 = unidecode.unidecode(date)\n",
    "    content = unidecode.unidecode(content)\n",
    "\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.cell(200, 10, txt=\"Title: \" + heading1, ln=True)\n",
    "    pdf.cell(200, 10, txt=\"Date: \" + date1, ln=True)\n",
    "    pdf.multi_cell(0, 10, txt=content)\n",
    "\n",
    "    file_name = \"temp.pdf\" # overwriting the files to optimise space complexity\n",
    "    full_path = path + file_name\n",
    "\n",
    "    pdf.output(full_path) # the pdf ready to be passed to the vdb\n",
    "    \n",
    "    # -------------------------- up to here the current pdf has just been created and is in ./data3/temp.pdf\n",
    "    \n",
    "    loaderv = PyMuPDFReader()\n",
    "    documents = loaderv.load(file_path=\"./data3/temp.pdf\")\n",
    "    \n",
    "    text_parser = SentenceSplitter(\n",
    "        chunk_size=1024,\n",
    "    )\n",
    "    \n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "    \n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(\n",
    "            text=text_chunk,\n",
    "        )\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        node.metadata[\"date\"] = date1\n",
    "        nodes.append(node)\n",
    "    \n",
    "    for node in nodes:\n",
    "        node_embedding = get_openai_embedding(node.get_content(metadata_mode=\"all\"))\n",
    "        node.embedding = node_embedding\n",
    "        \n",
    "    all_nodes_fin.extend(nodes)\n",
    "    hybrid_vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fin 2 bc previous crashes at a wrong url\n",
    "\n",
    "file_name = 'news_data_fin2.csv' # change to proper data\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "path = './data3/'\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"o-topper__headline o-topper__headline--large\", \"n-content-body js-article__content-body\"))\n",
    "all_nodes_fin2 = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    heading = row['Headline']\n",
    "    href_tag = row['Link']\n",
    "    date = row['Date']\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(href_tag, 'html.parser')\n",
    "    href = soup.find('a')['href']\n",
    "    \n",
    "    myurl = 'https://www.ft.com' + href\n",
    "    \n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(myurl,),\n",
    "        bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    content = docs[0].page_content\n",
    "    \n",
    "    heading1 = unidecode.unidecode(heading)\n",
    "    date1 = unidecode.unidecode(date)\n",
    "    content = unidecode.unidecode(content)\n",
    "\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.cell(200, 10, txt=\"Title: \" + heading1, ln=True)\n",
    "    pdf.cell(200, 10, txt=\"Date: \" + date1, ln=True)\n",
    "    pdf.multi_cell(0, 10, txt=content)\n",
    "\n",
    "    file_name = \"temp.pdf\" # overwriting the files to optimise space complexity\n",
    "    full_path = path + file_name\n",
    "\n",
    "    pdf.output(full_path) # the pdf ready to be passed to the vdb\n",
    "    \n",
    "    # -------------------------- up to here the current pdf has just been created and is in ./data3/temp.pdf\n",
    "    \n",
    "    loaderv = PyMuPDFReader()\n",
    "    documents = loaderv.load(file_path=\"./data3/temp.pdf\")\n",
    "    \n",
    "    text_parser = SentenceSplitter(\n",
    "        chunk_size=1024,\n",
    "    )\n",
    "    \n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "    \n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(\n",
    "            text=text_chunk,\n",
    "        )\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        node.metadata[\"date\"] = date1\n",
    "        nodes.append(node)\n",
    "    \n",
    "    for node in nodes:\n",
    "        node_embedding = get_openai_embedding(node.get_content(metadata_mode=\"all\"))\n",
    "        node.embedding = node_embedding\n",
    "        \n",
    "    all_nodes_fin2.extend(nodes)\n",
    "    hybrid_vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2083\n",
      "1582\n"
     ]
    }
   ],
   "source": [
    "print(len(all_nodes_fin))\n",
    "print(len(all_nodes_fin2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tech\n",
    "\n",
    "file_name = 'news_data_tech.csv' # change to proper data\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "path = './data3/'\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"o-topper__headline o-topper__headline--large\", \"n-content-body js-article__content-body\"))\n",
    "all_nodes_tech = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    heading = row['Headline']\n",
    "    href_tag = row['Link']\n",
    "    date = row['Date']\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(href_tag, 'html.parser')\n",
    "    href = soup.find('a')['href']\n",
    "    \n",
    "    myurl = 'https://www.ft.com' + href\n",
    "    \n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(myurl,),\n",
    "        bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    content = docs[0].page_content\n",
    "    \n",
    "    heading1 = unidecode.unidecode(heading)\n",
    "    date1 = unidecode.unidecode(date)\n",
    "    content = unidecode.unidecode(content)\n",
    "\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.cell(200, 10, txt=\"Title: \" + heading1, ln=True)\n",
    "    pdf.cell(200, 10, txt=\"Date: \" + date1, ln=True)\n",
    "    pdf.multi_cell(0, 10, txt=content)\n",
    "\n",
    "    file_name = \"temp.pdf\" # overwriting the files to optimise space complexity\n",
    "    full_path = path + file_name\n",
    "\n",
    "    pdf.output(full_path) # the pdf ready to be passed to the vdb\n",
    "    \n",
    "    # -------------------------- up to here the current pdf has just been created and is in ./data3/temp.pdf\n",
    "    \n",
    "    loaderv = PyMuPDFReader()\n",
    "    documents = loaderv.load(file_path=\"./data3/temp.pdf\")\n",
    "    \n",
    "    text_parser = SentenceSplitter(\n",
    "        chunk_size=1024,\n",
    "    )\n",
    "    \n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "    \n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(\n",
    "            text=text_chunk,\n",
    "        )\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        node.metadata[\"date\"] = date1\n",
    "        nodes.append(node)\n",
    "    \n",
    "    for node in nodes:\n",
    "        node_embedding = get_openai_embedding(node.get_content(metadata_mode=\"all\"))\n",
    "        node.embedding = node_embedding\n",
    "        \n",
    "    all_nodes_tech.extend(nodes)\n",
    "    hybrid_vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tech\n",
    "\n",
    "file_name = 'news_data_tech2.csv' # change to proper data\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "path = './data3/'\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"o-topper__headline o-topper__headline--large\", \"n-content-body js-article__content-body\"))\n",
    "all_nodes_tech2 = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    heading = row['Headline']\n",
    "    href_tag = row['Link']\n",
    "    date = row['Date']\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(href_tag, 'html.parser')\n",
    "    href = soup.find('a')['href']\n",
    "    \n",
    "    myurl = 'https://www.ft.com' + href\n",
    "    \n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(myurl,),\n",
    "        bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    content = docs[0].page_content\n",
    "    \n",
    "    heading1 = unidecode.unidecode(heading)\n",
    "    date1 = unidecode.unidecode(date)\n",
    "    content = unidecode.unidecode(content)\n",
    "\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.cell(200, 10, txt=\"Title: \" + heading1, ln=True)\n",
    "    pdf.cell(200, 10, txt=\"Date: \" + date1, ln=True)\n",
    "    pdf.multi_cell(0, 10, txt=content)\n",
    "\n",
    "    file_name = \"temp.pdf\" # overwriting the files to optimise space complexity\n",
    "    full_path = path + file_name\n",
    "\n",
    "    pdf.output(full_path) # the pdf ready to be passed to the vdb\n",
    "    \n",
    "    # -------------------------- up to here the current pdf has just been created and is in ./data3/temp.pdf\n",
    "    \n",
    "    loaderv = PyMuPDFReader()\n",
    "    documents = loaderv.load(file_path=\"./data3/temp.pdf\")\n",
    "    \n",
    "    text_parser = SentenceSplitter(\n",
    "        chunk_size=1024,\n",
    "    )\n",
    "    \n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "    \n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(\n",
    "            text=text_chunk,\n",
    "        )\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        node.metadata[\"date\"] = date1\n",
    "        nodes.append(node)\n",
    "    \n",
    "    for node in nodes:\n",
    "        node_embedding = get_openai_embedding(node.get_content(metadata_mode=\"all\"))\n",
    "        node.embedding = node_embedding\n",
    "        \n",
    "    all_nodes_tech2.extend(nodes)\n",
    "    hybrid_vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Date: September 15, 2023\n",
      "Maximum Date: July 29, 2024\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "dates = [datetime.strptime(node.metadata[\"date\"], \"%B %d, %Y\") for node in all_nodes]  # Adjusted date format\n",
    "\n",
    "# Find the minimum and maximum dates\n",
    "min_date = min(dates)\n",
    "max_date = max(dates)\n",
    "\n",
    "print(f\"Minimum Date: {min_date.strftime('%B %d, %Y')}\")\n",
    "print(f\"Maximum Date: {max_date.strftime('%B %d, %Y')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2863\n",
      "2368\n",
      "2613\n"
     ]
    }
   ],
   "source": [
    "# print(len(all_nodes))\n",
    "# print(len(all_nodes_tech))\n",
    "# print(len(all_nodes_tech2))\n",
    "\n",
    "# 2863 2368 2613 2083 1582, 12k nodes ish\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RETRIEVAL - dont rerun previous cells now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "query_str = \"Who bought Marathon Oil?\" # ConocoPhilips\n",
    "query_embedding = get_openai_embedding(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vector store query\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "query_mode = \"default\" # sparse or hybrid\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = hybrid_vector_store.query(vector_store_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_result.nodes):\n",
    "    score: Optional[float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "    nodes_with_scores.append(NodeWithScore(node=node, score=score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided, the prediction for the stock price movement of ConocoPhillips would likely be up. The mention of energy being favored, along with the advantage of a permanent energy input cost advantage for US industrials after the shale revolution, suggests a positive outlook for companies in the energy sector like ConocoPhillips.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = get_openai_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = hybrid_vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores\n",
    "\n",
    "retriever_old = VectorDBRetriever(\n",
    "    hybrid_vector_store, embed_model.get_embedding, query_mode=\"default\", similarity_top_k=2\n",
    ")\n",
    "\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "query_engine_old = RetrieverQueryEngine(\n",
    "    retriever=retriever_old,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    # node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "\n",
    "### HERE, IS BETTER, there is clear data leakage though\n",
    "\n",
    "# response = query_engine_old.query(\"Summary of stock price change relevant information of ConocoPhilips up to December 2023.\")\n",
    "response = query_engine_old.query(\"whats your prediction for stock price movement of conocophilips based on the info you have on them and its industry? up or down?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExxonMobil has announced plans to increase capital spending over the next few years to boost oil and gas production and expand its low-carbon energy division. This strategic shift follows a period of reduced spending after a commodity price crash in 2020. The company aims to raise its oil and gas production by about 10% by 2027 through increased investments in key operations. Additionally, Exxon plans to invest significantly in low-carbon projects, focusing on areas like lithium, hydrogen, biofuels, and carbon capture and storage. The company's stock price trend may be influenced by these initiatives, as well as broader industry trends and global events impacting the energy sector.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List, Optional\n",
    "from datetime import datetime \n",
    "\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "    except ValueError:\n",
    "        return None  # Handle error appropriately\n",
    "        \n",
    "class VectorDBRetriever2(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "    def _retrieve(self, query_bundle: QueryBundle, filter_date: Optional[str] = None) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = get_openai_embedding(query_bundle.query_str)\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = hybrid_vector_store.query(vector_store_query)\n",
    "        \n",
    "        filter_datetime = parse_date(filter_date) if filter_date else None\n",
    "        # print(f\"Filter Date: {filter_date}, Parsed Filter Date: {filter_datetime}\")\n",
    "\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            node_date_str = node.metadata.get(\"date\")\n",
    "            node_date = parse_date(node_date_str) if node_date_str else None\n",
    "\n",
    "            if node_date and (not filter_datetime or node_date < filter_datetime):\n",
    "                score: Optional[float] = None\n",
    "                if query_result.similarities is not None:\n",
    "                    score = query_result.similarities[index]\n",
    "                nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores\n",
    "\n",
    "retriever = VectorDBRetriever2(\n",
    "    hybrid_vector_store, embed_model.get_embedding, query_mode=\"default\", similarity_top_k=2\n",
    ")\n",
    "\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "from llama_index.core.callbacks.schema import CBEventType, EventPayload\n",
    "from llama_index.core.base.response.schema import RESPONSE_TYPE\n",
    "\n",
    "\n",
    "\n",
    "class CRetrieverQueryEngine(RetrieverQueryEngine):\n",
    "    # Other parts of the class remain unchanged\n",
    "    def __init__(self, retriever, response_synthesizer, node_postprocessors=None):\n",
    "        super().__init__(retriever=retriever, response_synthesizer=response_synthesizer, node_postprocessors=node_postprocessors)\n",
    "        \n",
    "    def _query(self, query_bundle: QueryBundle, filter_date: Optional[str] = None) -> RESPONSE_TYPE:\n",
    "        \"\"\"Answer a query.\"\"\"\n",
    "        with self.callback_manager.event(\n",
    "            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n",
    "        ) as query_event:\n",
    "            nodes = self._retriever._retrieve(query_bundle, filter_date)\n",
    "            response = self._response_synthesizer.synthesize(\n",
    "                query=query_bundle,\n",
    "                nodes=nodes,\n",
    "            )\n",
    "            query_event.on_end(payload={EventPayload.RESPONSE: response})\n",
    "\n",
    "        return response\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "query_engine = CRetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "query_bundle = QueryBundle(query_str=\"Give a summary of information relevant to the stock price trend of Exxon. Consider the industry and global events that may influence it. Today is July 1st 2024, and I will want to predict the stock price movement for tomorrow, so consider the time when reporting the relevance of the news.\")\n",
    "\n",
    "response = query_engine._query(query_bundle, filter_date=\"July 1, 2024\")\n",
    "print(response)\n",
    "\n",
    "# query_embedding = get_openai_embedding(query_bundle.query_str)\n",
    "# print(f\"Embedding for '{query_bundle.query_str}': {query_embedding}\")\n",
    "\n",
    "# query_result = hybrid_vector_store.query(vector_store_query)\n",
    "# print(f\"Query result nodes count: {len(query_result.nodes)}\")\n",
    "\n",
    "# print(f\"Synthesized response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector_stores\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     MetadataFilter,\n\u001b[1;32m      7\u001b[0m     MetadataFilters,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m index \u001b[38;5;241m=\u001b[39m VectorStoreIndex\u001b[38;5;241m.\u001b[39mfrom_vector_store(vector_store\u001b[38;5;241m=\u001b[39mhybrid_vector_store)\n\u001b[0;32m---> 10\u001b[0m index\u001b[38;5;241m.\u001b[39minsert_nodes(\u001b[43mall_nodes\u001b[49m)\n\u001b[1;32m     11\u001b[0m index\u001b[38;5;241m.\u001b[39minsert_nodes(all_nodes_fin)\n\u001b[1;32m     12\u001b[0m index\u001b[38;5;241m.\u001b[39minsert_nodes(all_nodes_fin2)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_nodes' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    ")\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=hybrid_vector_store)\n",
    "index.insert_nodes(all_nodes)\n",
    "index.insert_nodes(all_nodes_fin)\n",
    "index.insert_nodes(all_nodes_fin2)\n",
    "index.insert_nodes(all_nodes_tech)\n",
    "index.insert_nodes(all_nodes_tech2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_pages': 2, 'file_path': './data3/temp.pdf', 'source': '2', 'date': 'July 11, 2024'}\n",
      "{'total_pages': 2, 'file_path': './data3/temp.pdf', 'source': '1', 'date': 'March 4, 2024'}\n",
      "No, there is no information provided in the context about ConocoPhilips buying Marathon Oil.\n"
     ]
    }
   ],
   "source": [
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=\"date\", value=\"June 4, 2024\", operator=\"<=\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=10,\n",
    "    # filters=filters,\n",
    ")\n",
    "\n",
    "retrieved_nodes = retriever.retrieve(\"Did ConocoPhilips buy Marathon Oil?\")\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    print(node.node.metadata)\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    # node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"Did ConocoPhilips buy Marathon Oil?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
