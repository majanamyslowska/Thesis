{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE A VECTOR DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a list of articles (link, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    # Energy Sector\n",
    "    \"XOM\", \"CVX\", \"COP\", \"EOG\", \"SLB\",\n",
    "    \"Exxon\", \"Chevron\", \"ConocoPhillips\", \"EOG Resources Inc\", \"Schlumberger NV\",\n",
    "    \"oil\", \"gas\", \"energy\", \"OPEC\", \"power\",\n",
    "    \"electricity\", \"green\", \"utilities\",\n",
    "\n",
    "    # Financials Sector\n",
    "    \"JPM\", \"BAC\", \"WFC\", \"AXP\", \"MS\",\n",
    "    \"JPMorgan\", \"Bank of America\", \"Wells Fargo\", \"American Express\", \"Morgan Stanley\",\n",
    "    \"bank\", \"interest rates\", \"savings\", \"investment\", \"regulation\",\n",
    "    \"inflation\", \"employment\", \"stock\", \"bond\", \"FED\",\n",
    "    \"SEC\", \"NYSE\", \"NASDAQ\", \"S&P500\",\n",
    "\n",
    "    # Tech Sector\n",
    "    \"MSFT\", \"AAPL\", \"NVDA\", \"GOOGL\", \"META\",\n",
    "    \"Microsoft\", \"Apple\", \"Nvidia\", \"Alphabet\", \"Meta\",\n",
    "    \"AI\", \"Google\", \"cybersecurity\", \"fintech\", \"data\", \"cloud\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Status Code: 200\n",
      "2\n",
      "Total articles collected in news_data: 7\n",
      "[['Is ChatGPT good at day trading? Probably not', '/content/0608b6a0-f586-430c-9e68-a177c544435a', 'March 28, 2024'], ['GraniteShares files to increase leverage on 17 ETFs', '/content/f26c4b3b-baa4-4586-9fa2-bf8d63fd457b', 'November 30, 2023'], ['Microsoft, and a macro lesson', '/content/d5ba4cab-7976-4352-b624-7654c6acf7c3', 'October 28, 2021'], ['High expectations leave no room for errorPremiumcontent', '/content/ed5c04dd-75cf-4338-b51c-869affebedde', 'July 7, 2020'], ['Markets not live, Thursday 30th January 2020', '/content/ad45f2d9-07e6-4b0c-a040-f831f8af5ae8', 'January 30, 2020'], ['Chinaâ€™s credit pulse has a kickPremiumcontent', '/content/3c35f688-36f6-11ea-a6d3-9a26f8c3cba4', 'January 14, 2020'], ['Investors Chronicle: Tesco, Serco, Apple', '/content/6fbc606c-1359-11e9-a581-4ff78404524e', 'January 11, 2019']]\n"
     ]
    }
   ],
   "source": [
    "# ft articles data [heading, link, date] for a hardcoded keyword\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "news_data = []\n",
    "keyword = 'aapl'\n",
    "cutoff_date = datetime.strptime(\"01-Dec-2018\", \"%d-%b-%Y\")\n",
    "\n",
    "keyurl = f\"https://www.ft.com/search?q={keyword}&page=1&sort=date&isFirstView=true\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://www.ft.com/',\n",
    "    'Origin': 'https://www.ft.com'\n",
    "}\n",
    "\n",
    "keyresult = requests.get(keyurl, headers=headers)\n",
    "# keyresult = requests.get(keyurl)\n",
    "print(f\"HTTP Status Code: {keyresult.status_code}\")\n",
    "\n",
    "keyresult_content = keyresult.content\n",
    "keysoup = BeautifulSoup(keyresult_content, \"html.parser\")\n",
    "# print(keysoup.prettify())\n",
    "maxpages = keysoup.find('span', {\"class\": \"search-pagination__page\"})\n",
    "maxpages = int(maxpages.get_text(strip=True).split()[-1])\n",
    "# maxpages += 5\n",
    "print(maxpages)\n",
    "\n",
    "\n",
    "for page in range(1, maxpages):\n",
    "    url = f\"https://www.ft.com/search?q={keyword}&page={page}&sort=date&isFirstView=true\"\n",
    "    result = requests.get(url, headers=headers)\n",
    "    # print(page)\n",
    "    # print(result.text)\n",
    "    if \"Sorry\" in result.text: # maybe unnecessary\n",
    "        print(f\"Stopping iteration: Page {page} is not available.\")\n",
    "        break\n",
    "    \n",
    "    result_content = result.content\n",
    "    soup = BeautifulSoup(result_content, \"lxml\")\n",
    "    \n",
    "    for article in soup.findAll(\"div\", {\"class\": \"o-teaser\"}):\n",
    "        heading = article.find(\"div\", {\"class\": \"o-teaser__heading\"}).get_text(strip=True) #find(text=True), find text is outdated\n",
    "        link = article.find(\"div\", {\"class\": \"o-teaser__heading\"}).find('a', href=True)\n",
    "        date = article.find('time', {\"class\": \"o-teaser__timestamp-date\"})\n",
    "        \n",
    "        # print(heading, link, date)\n",
    "        if heading and link and date:\n",
    "            article_date = datetime.strptime(date.text.strip(), \"%B %d, %Y\")\n",
    "            if article_date > cutoff_date:\n",
    "                news_data.append([heading, link['href'], date.text.strip()])\n",
    "\n",
    "# print(pages)\n",
    "\n",
    "print(f\"Total articles collected in news_data: {len(news_data)}\")\n",
    "# print(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup news_data, dont refresh\n",
    "\n",
    "news_data = []\n",
    "collected_links = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup keywords\n",
    "\n",
    "# cutoff_date = datetime.strptime(\"15-Sep-2023\", \"%d-%b-%Y\")\n",
    "\n",
    "# keywords = [\n",
    "#     # Energy Sector\n",
    "#     \"XOM\", \"CVX\", \"EOG\", \"SLB\",\n",
    "#     \"Exxon\", \"Chevron\", \"ConocoPhillips\", \"Schlumberger\",\n",
    "#     \"oil\", \"gas\", \"energy\", \"OPEC\",\n",
    "#     \"electricity\", \"green\", \"utilities\",\n",
    "\n",
    "#     # Financials Sector\n",
    "#     \"JPM\", \"BAC\", \"WFC\", \"AXP\",\n",
    "#     \"JPMorgan\", \"Bank of America\", \"Wells Fargo\", \"American Express\", \"Morgan Stanley\",\n",
    "#     \"interest rates\", \"savings\", \"investment\", \"regulation\",\n",
    "#     \"inflation\", \"employment\", \"FED\", \"SEC\", \"NYSE\", \"NASDAQ\",\n",
    "\n",
    "#     # Tech Sector\n",
    "#     \"MSFT\", \"AAPL\", \"NVDA\", \"GOOGL\", \"META\",\n",
    "#     \"Microsoft\", \"Apple\", \"Nvidia\", \"Alphabet\", \"Meta\",\n",
    "#     \"AI\", \"Google\", \"cybersecurity\", \"fintech\", \"data\", \"cloud\"\n",
    "# ]\n",
    "\n",
    "# k1 = [\n",
    "#     \"XOM\", \"CVX\", \"EOG\", \"SLB\" # energy 1  \n",
    "# ]\n",
    "# k2 = [\n",
    "#     \"Exxon\", \"Chevron\", \"ConocoPhillips\", \"Schlumberger\" # energy 2\n",
    "# ]\n",
    "# k3 = [\n",
    "#     \"oil\", \"gas\", \"energy\", \"OPEC\" # energy 3\n",
    "# ]\n",
    "# k4 = [\n",
    "#     \"electricity\", \"green\", \"utilities\" # energy 4\n",
    "# ]\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://www.ft.com/',\n",
    "    'Origin': 'https://www.ft.com'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     80\u001b[0m result_content \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m---> 81\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m skip_to_next_keyword \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     84\u001b[0m page_data \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bs4/__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bs4/__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 478\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bs4/builder/_lxml.py:379\u001b[0m, in \u001b[0;36mLXMLTreeBuilder.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    377\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\u001b[38;5;241m.\u001b[39moriginal_encoding\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mfeed(markup)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bs4/builder/_lxml.py:124\u001b[0m, in \u001b[0;36mLXMLTreeBuilderForXML.parser_for\u001b[0;34m(self, encoding)\u001b[0m\n\u001b[1;32m    120\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_parser(encoding)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parser, Callable):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# Instantiate the parser with default arguments\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     parser \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrip_cdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecover\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ENERGY\n",
    "# ft articles data [heading, link, date] for a list of keywords, need to split the keywords list bc it crashes\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://www.ft.com/',\n",
    "    'Origin': 'https://www.ft.com'\n",
    "}\n",
    "\n",
    "k1 = [\n",
    "    \"XOM\", \"CVX\", \"EOG\", \"SLB\" # energy 1  \n",
    "]\n",
    "k2 = [\n",
    "    \"Exxon\", \"Chevron\", \"ConocoPhillips\", \"Schlumberger\" # energy 2\n",
    "]\n",
    "k3 = [\n",
    "    \"oil\", \"gas\", \"energy\", \"OPEC\" # energy 3\n",
    "]\n",
    "k4 = [\n",
    "    \"electricity\", \"green\", \"utilities\" # energy 4\n",
    "]\n",
    "# collected_links = set()\n",
    "file_name = 'news_data_energy.csv'\n",
    "\n",
    "def write_to_csv(data, file_name):\n",
    "    file_exists = os.path.isfile(file_name)\n",
    "    with open(file_name, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            # Write header only if the file does not exist\n",
    "            writer.writerow([\"Headline\", \"Link\", \"Date\"])\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def read_existing_links(file_name):\n",
    "    if not os.path.isfile(file_name):\n",
    "        return set()\n",
    "    with open(file_name, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader, None)  # Skip the header\n",
    "        existing_links = {row[1] for row in reader}\n",
    "    return existing_links\n",
    "\n",
    "existing_links = read_existing_links(file_name)\n",
    "cutoff_date = datetime.strptime(\"15-Sep-2023\", \"%d-%b-%Y\")\n",
    "\n",
    "\n",
    "for keyword in k4:\n",
    "    keyurl = f\"https://www.ft.com/search?q={keyword}&page=1&sort=date&isFirstView=true\"\n",
    "\n",
    "\n",
    "    keyresult = requests.get(keyurl, headers=headers)\n",
    "\n",
    "    keyresult_content = keyresult.content\n",
    "    keysoup = BeautifulSoup(keyresult_content, \"html.parser\")\n",
    "    try:\n",
    "        maxpages = keysoup.find('span', {\"class\": \"search-pagination__page\"})\n",
    "        maxpages = int(maxpages.get_text(strip=True).split()[-1])\n",
    "    except (AttributeError, ValueError) as e:\n",
    "        print(f\"Skipping keyword '{keyword}' due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "    for page in range(1, maxpages):\n",
    "        url = f\"https://www.ft.com/search?q={keyword}&page={page}&sort=date&isFirstView=true\"\n",
    "        result = requests.get(url, headers=headers)\n",
    "        \n",
    "        if \"Sorry\" in result.text: # maybe unnecessary\n",
    "            print(f\"Stopping iteration: Page {page} is not available.\")\n",
    "            break\n",
    "        \n",
    "        result_content = result.content\n",
    "        soup = BeautifulSoup(result_content, \"lxml\")\n",
    "        \n",
    "        skip_to_next_keyword = False\n",
    "        page_data = []\n",
    "        \n",
    "        for article in soup.findAll(\"div\", {\"class\": \"o-teaser\"}):\n",
    "            heading = article.find(\"div\", {\"class\": \"o-teaser__heading\"}).get_text(strip=True)\n",
    "            link = article.find(\"div\", {\"class\": \"o-teaser__heading\"}).find('a', href=True)\n",
    "            date = article.find('time', {\"class\": \"o-teaser__timestamp-date\"})\n",
    "            \n",
    "            if heading and link and date:\n",
    "                \n",
    "                article_date = datetime.strptime(date.text.strip(), \"%B %d, %Y\")\n",
    "                if article_date < cutoff_date:\n",
    "                    skip_to_next_keyword = True\n",
    "                    break\n",
    "                if link not in existing_links:\n",
    "                    page_data.append([heading, link, date.text.strip()])\n",
    "                    existing_links.add(link)\n",
    "        if page_data:\n",
    "            write_to_csv(page_data, file_name)\n",
    "        if skip_to_next_keyword:\n",
    "            break\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, maxpages):\n\u001b[1;32m     78\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.ft.com/search?q=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&sort=date&isFirstView=true\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 79\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSorry\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mtext: \u001b[38;5;66;03m# maybe unnecessary\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopping iteration: Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/requests/adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    586\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:1423\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1422\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1423\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1425\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# FINANCE\n",
    "# ft articles data [heading, link, date] for a list of keywords, need to split the keywords list bc it crashes\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://www.ft.com/',\n",
    "    'Origin': 'https://www.ft.com'\n",
    "}\n",
    "# Financials Sector\n",
    "#     \"JPM\", \"BAC\", \"WFC\", \"AXP\",\n",
    "#     \"JPMorgan\", \"Bank of America\", \"Wells Fargo\", \"American Express\", \"Morgan Stanley\",\n",
    "#     \"interest rates\", \"savings\", \"investment\", \"regulation\",\n",
    "#     \"inflation\", \"employment\", \"FED\", \"SEC\", \"NYSE\", \"NASDAQ\",\n",
    "k1 = [\n",
    "    \"JPM\", \"BAC\", \"WFC\", \"AXP\" \n",
    "]\n",
    "k2 = [\n",
    "    \"JPMorgan\", \"Bank of America\", \"Wells Fargo\", \"American Express\", \"Morgan Stanley\"\n",
    "]\n",
    "k3 = [\n",
    "    \"interest rates\", \"savings\", \"investment\", \"regulation\"\n",
    "]\n",
    "k4 = [\n",
    "    \"inflation\", \"employment\", \"FED\", \"SEC\", \"NYSE\", \"NASDAQ\"\n",
    "]\n",
    "# collected_links = set()\n",
    "file_name = 'news_data_fin.csv'\n",
    "\n",
    "def write_to_csv(data, file_name):\n",
    "    file_exists = os.path.isfile(file_name)\n",
    "    with open(file_name, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            # Write header only if the file does not exist\n",
    "            writer.writerow([\"Headline\", \"Link\", \"Date\"])\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def read_existing_links(file_name):\n",
    "    if not os.path.isfile(file_name):\n",
    "        return set()\n",
    "    with open(file_name, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader, None)  # Skip the header\n",
    "        existing_links = {row[1] for row in reader}\n",
    "    return existing_links\n",
    "\n",
    "existing_links = read_existing_links(file_name)\n",
    "cutoff_date = datetime.strptime(\"15-Sep-2023\", \"%d-%b-%Y\")\n",
    "\n",
    "\n",
    "for keyword in k4:\n",
    "    keyurl = f\"https://www.ft.com/search?q={keyword}&page=1&sort=date&isFirstView=true\"\n",
    "\n",
    "\n",
    "    keyresult = requests.get(keyurl, headers=headers)\n",
    "\n",
    "    keyresult_content = keyresult.content\n",
    "    keysoup = BeautifulSoup(keyresult_content, \"html.parser\")\n",
    "    try:\n",
    "        maxpages = keysoup.find('span', {\"class\": \"search-pagination__page\"})\n",
    "        maxpages = int(maxpages.get_text(strip=True).split()[-1])\n",
    "    except (AttributeError, ValueError) as e:\n",
    "        print(f\"Skipping keyword '{keyword}' due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "    for page in range(1, maxpages):\n",
    "        url = f\"https://www.ft.com/search?q={keyword}&page={page}&sort=date&isFirstView=true\"\n",
    "        result = requests.get(url, headers=headers)\n",
    "        \n",
    "        if \"Sorry\" in result.text: # maybe unnecessary\n",
    "            print(f\"Stopping iteration: Page {page} is not available.\")\n",
    "            break\n",
    "        \n",
    "        result_content = result.content\n",
    "        soup = BeautifulSoup(result_content, \"lxml\")\n",
    "        \n",
    "        skip_to_next_keyword = False\n",
    "        page_data = []\n",
    "        \n",
    "        for article in soup.findAll(\"div\", {\"class\": \"o-teaser\"}):\n",
    "            heading = article.find(\"div\", {\"class\": \"o-teaser__heading\"}).get_text(strip=True)\n",
    "            link = article.find(\"div\", {\"class\": \"o-teaser__heading\"}).find('a', href=True)\n",
    "            date = article.find('time', {\"class\": \"o-teaser__timestamp-date\"})\n",
    "            \n",
    "            if heading and link and date:\n",
    "                \n",
    "                article_date = datetime.strptime(date.text.strip(), \"%B %d, %Y\")\n",
    "                if article_date < cutoff_date:\n",
    "                    skip_to_next_keyword = True\n",
    "                    break\n",
    "                if link not in existing_links:\n",
    "                    page_data.append([heading, link, date.text.strip()])\n",
    "                    existing_links.add(link)\n",
    "        if page_data:\n",
    "            write_to_csv(page_data, file_name)\n",
    "        if skip_to_next_keyword:\n",
    "            break\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# TECH\n",
    "# ft articles data [heading, link, date] for a list of keywords, need to split the keywords list bc it crashes\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://www.ft.com/',\n",
    "    'Origin': 'https://www.ft.com'\n",
    "}\n",
    "#     # Tech Sector\n",
    "#     \"MSFT\", \"AAPL\", \"NVDA\", \"GOOGL\", \"META\",\n",
    "#     \"Microsoft\", \"Apple\", \"Nvidia\", \"Alphabet\", \"Meta\",\n",
    "#     \"AI\", \"Google\", \"cybersecurity\", \"fintech\", \"data\", \"cloud\"\n",
    "k1 = [\n",
    "    \"MSFT\", \"AAPL\", \"NVDA\", \"GOOGL\", \"META\"\n",
    "]\n",
    "k2 = [\n",
    "    \"Microsoft\", \"Apple\", \"Nvidia\", \"Alphabet\"\n",
    "]\n",
    "k3 = [\n",
    "    \"Meta\", \"AI\"\n",
    "]\n",
    "k4 = [\n",
    "    \"cybersecurity\", \"fintech\", \"data\", \"cloud\"\n",
    "]\n",
    "\n",
    "# collected_links = set()\n",
    "file_name = 'news_data_tech.csv'\n",
    "\n",
    "def write_to_csv(data, file_name):\n",
    "    file_exists = os.path.isfile(file_name)\n",
    "    with open(file_name, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            # Write header only if the file does not exist\n",
    "            writer.writerow([\"Headline\", \"Link\", \"Date\"])\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def read_existing_links(file_name):\n",
    "    if not os.path.isfile(file_name):\n",
    "        return set()\n",
    "    with open(file_name, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader, None)  # Skip the header\n",
    "        existing_links = {row[1] for row in reader}\n",
    "    return existing_links\n",
    "\n",
    "existing_links = read_existing_links(file_name)\n",
    "cutoff_date = datetime.strptime(\"15-Sep-2023\", \"%d-%b-%Y\")\n",
    "\n",
    "\n",
    "for keyword in k4:\n",
    "    keyurl = f\"https://www.ft.com/search?q={keyword}&page=1&sort=date&isFirstView=true\"\n",
    "\n",
    "\n",
    "    keyresult = requests.get(keyurl, headers=headers)\n",
    "\n",
    "    keyresult_content = keyresult.content\n",
    "    keysoup = BeautifulSoup(keyresult_content, \"html.parser\")\n",
    "    try:\n",
    "        maxpages = keysoup.find('span', {\"class\": \"search-pagination__page\"})\n",
    "        maxpages = int(maxpages.get_text(strip=True).split()[-1])\n",
    "    except (AttributeError, ValueError) as e:\n",
    "        print(f\"Skipping keyword '{keyword}' due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "    for page in range(1, maxpages):\n",
    "        url = f\"https://www.ft.com/search?q={keyword}&page={page}&sort=date&isFirstView=true\"\n",
    "        result = requests.get(url, headers=headers)\n",
    "        \n",
    "        if \"Sorry\" in result.text: # maybe unnecessary\n",
    "            print(f\"Stopping iteration: Page {page} is not available.\")\n",
    "            break\n",
    "        \n",
    "        result_content = result.content\n",
    "        soup = BeautifulSoup(result_content, \"lxml\")\n",
    "        \n",
    "        skip_to_next_keyword = False\n",
    "        page_data = []\n",
    "        \n",
    "        for article in soup.findAll(\"div\", {\"class\": \"o-teaser\"}):\n",
    "            heading = article.find(\"div\", {\"class\": \"o-teaser__heading\"}).get_text(strip=True)\n",
    "            link = article.find(\"div\", {\"class\": \"o-teaser__heading\"}).find('a', href=True)\n",
    "            date = article.find('time', {\"class\": \"o-teaser__timestamp-date\"})\n",
    "            \n",
    "            if heading and link and date:\n",
    "                \n",
    "                article_date = datetime.strptime(date.text.strip(), \"%B %d, %Y\")\n",
    "                if article_date < cutoff_date:\n",
    "                    skip_to_next_keyword = True\n",
    "                    break\n",
    "                if link not in existing_links:\n",
    "                    page_data.append([heading, link, date.text.strip()])\n",
    "                    existing_links.add(link)\n",
    "        if page_data:\n",
    "            write_to_csv(page_data, file_name)\n",
    "        if skip_to_next_keyword:\n",
    "            break\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n"
     ]
    }
   ],
   "source": [
    "print(len(news_data)) #110 rows after k1, 851\n",
    "# k4 fully finished, energy 4745 rows \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Date  Count\n",
      "587      May 20, 2020      1\n",
      "588      May 17, 2020      1\n",
      "589      May 14, 2020      1\n",
      "590      May 13, 2020      1\n",
      "591      May 12, 2020      1\n",
      "..                ...    ...\n",
      "979  December 6, 2018      1\n",
      "980  December 3, 2018      1\n",
      "981    August 4, 2023      1\n",
      "982     July 24, 2023      1\n",
      "983      July 3, 2023      1\n",
      "\n",
      "[365 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "file_name = 'news_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Group by the \"Date\" column and count the occurrences\n",
    "date_counts = df['Date'].value_counts().reset_index()\n",
    "date_counts.columns = ['Date', 'Count']\n",
    "\n",
    "# Print the results\n",
    "# print(date_counts)\n",
    "\n",
    "# Sort the date_counts DataFrame by 'Count' in descending order\n",
    "date_counts_sorted = date_counts.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Get the last 365 rows\n",
    "last_365_rows = date_counts_sorted.tail(365)\n",
    "print(last_365_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries for 2019: 205\n"
     ]
    }
   ],
   "source": [
    "file_name = 'news_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Convert the 'Date' column to datetime format for processing\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%B %d, %Y')\n",
    "\n",
    "# Filter the DataFrame for dates in 2019\n",
    "df_2019 = df[df['Date'].dt.year == 2019]\n",
    "\n",
    "# Calculate the count of entries for 2019\n",
    "count_2019 = df_2019.shape[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of entries for 2019: {count_2019}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year\n",
      "2019     205\n",
      "2020     184\n",
      "2021     223\n",
      "2022     309\n",
      "2023     558\n",
      "2024    3255\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "file_name = 'news_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%B %d, %Y')\n",
    "\n",
    "# Extract the year from the 'Date' column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Filter the DataFrame for the years 2019 to 2024\n",
    "filtered_df = df[df['Year'].between(2019, 2024)]\n",
    "\n",
    "# Group by the 'Year' column and count the occurrences\n",
    "year_counts = filtered_df['Year'].value_counts().sort_index()\n",
    "\n",
    "# Print the results\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  Count\n",
      "0   2024-07-17     85\n",
      "1   2024-07-19     72\n",
      "2   2024-06-26     70\n",
      "3   2024-07-18     64\n",
      "4   2024-07-23     61\n",
      "..         ...    ...\n",
      "192 2024-01-04      1\n",
      "193 2024-02-24      1\n",
      "194 2024-01-10      1\n",
      "195 2024-02-05      1\n",
      "196 2024-03-02      1\n",
      "\n",
      "[197 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "file_name = 'news_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%B %d, %Y')\n",
    "\n",
    "# Filter the DataFrame for the year 2024\n",
    "df_2024 = df[df['Date'].dt.year == 2024]\n",
    "\n",
    "# Group by the 'Date' column and count the occurrences\n",
    "date_counts_2024 = df_2024['Date'].value_counts().reset_index()\n",
    "date_counts_2024.columns = ['Date', 'Count']\n",
    "\n",
    "# Sort the date_counts DataFrame by 'Count' in descending order\n",
    "date_counts_2024_sorted = date_counts_2024.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Get the last 365 rows\n",
    "last_365_rows_2024 = date_counts_2024_sorted.tail(365)\n",
    "\n",
    "# Print the results\n",
    "print(last_365_rows_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YearMonth\n",
      "2023-10     68\n",
      "2023-11     53\n",
      "2023-12    105\n",
      "2024-01    111\n",
      "2024-02    181\n",
      "2024-03    149\n",
      "2024-04    264\n",
      "2024-05    647\n",
      "2024-06    748\n",
      "Freq: M, Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w9/qwhj77nd0dn8pf5px6zk_xxw0000gn/T/ipykernel_40316/3684213754.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['YearMonth'] = filtered_df['Date'].dt.to_period('M')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "file_name = 'news_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%B %d, %Y')\n",
    "\n",
    "# Filter the DataFrame for the months from October 2023 to July 2024\n",
    "start_date = '2023-10-01'\n",
    "end_date = '2024-06-30'\n",
    "filtered_df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]\n",
    "\n",
    "# Group by year and month, then count the occurrences\n",
    "filtered_df['YearMonth'] = filtered_df['Date'].dt.to_period('M')\n",
    "month_counts = filtered_df['YearMonth'].value_counts().sort_index()\n",
    "\n",
    "# Print the results\n",
    "print(month_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
