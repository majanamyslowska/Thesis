{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE A VECTOR DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a list of articles (link, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    # Energy Sector\n",
    "    \"XOM\", \"CVX\", \"COP\", \"EOG\", \"SLB\",\n",
    "    \"Exxon\", \"Chevron\", \"ConocoPhillips\", \"EOG Resources Inc\", \"Schlumberger NV\",\n",
    "    \"oil\", \"gas\", \"energy\", \"OPEC\", \"power\",\n",
    "    \"electricity\", \"green\", \"utilities\",\n",
    "\n",
    "    # Financials Sector\n",
    "    \"JPM\", \"BAC\", \"WFC\", \"AXP\", \"MS\",\n",
    "    \"JPMorgan\", \"Bank of America\", \"Wells Fargo\", \"American Express\", \"Morgan Stanley\",\n",
    "    \"bank\", \"interest rates\", \"savings\", \"investment\", \"regulation\",\n",
    "    \"inflation\", \"employment\", \"stock\", \"bond\", \"FED\",\n",
    "    \"SEC\", \"NYSE\", \"NASDAQ\", \"S&P500\",\n",
    "\n",
    "    # Tech Sector\n",
    "    \"MSFT\", \"AAPL\", \"NVDA\", \"GOOGL\", \"META\",\n",
    "    \"Microsoft\", \"Apple\", \"Nvidia\", \"Alphabet\", \"Meta\",\n",
    "    \"AI\", \"Google\", \"cybersecurity\", \"fintech\", \"data\", \"cloud\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Status Code: 200\n",
      "2\n",
      "Total articles collected in news_data: 7\n",
      "[['Is ChatGPT good at day trading? Probably not', '/content/0608b6a0-f586-430c-9e68-a177c544435a', 'March 28, 2024'], ['GraniteShares files to increase leverage on 17 ETFs', '/content/f26c4b3b-baa4-4586-9fa2-bf8d63fd457b', 'November 30, 2023'], ['Microsoft, and a macro lesson', '/content/d5ba4cab-7976-4352-b624-7654c6acf7c3', 'October 28, 2021'], ['High expectations leave no room for errorPremiumcontent', '/content/ed5c04dd-75cf-4338-b51c-869affebedde', 'July 7, 2020'], ['Markets not live, Thursday 30th January 2020', '/content/ad45f2d9-07e6-4b0c-a040-f831f8af5ae8', 'January 30, 2020'], ['Chinaâ€™s credit pulse has a kickPremiumcontent', '/content/3c35f688-36f6-11ea-a6d3-9a26f8c3cba4', 'January 14, 2020'], ['Investors Chronicle: Tesco, Serco, Apple', '/content/6fbc606c-1359-11e9-a581-4ff78404524e', 'January 11, 2019']]\n"
     ]
    }
   ],
   "source": [
    "# ft articles data [heading, link, date] for a hardcoded keyword\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "news_data = []\n",
    "keyword = 'aapl'\n",
    "cutoff_date = datetime.strptime(\"01-Dec-2018\", \"%d-%b-%Y\")\n",
    "\n",
    "keyurl = f\"https://www.ft.com/search?q={keyword}&page=1&sort=date&isFirstView=true\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://www.ft.com/',\n",
    "    'Origin': 'https://www.ft.com'\n",
    "}\n",
    "\n",
    "keyresult = requests.get(keyurl, headers=headers)\n",
    "# keyresult = requests.get(keyurl)\n",
    "print(f\"HTTP Status Code: {keyresult.status_code}\")\n",
    "\n",
    "keyresult_content = keyresult.content\n",
    "keysoup = BeautifulSoup(keyresult_content, \"html.parser\")\n",
    "# print(keysoup.prettify())\n",
    "maxpages = keysoup.find('span', {\"class\": \"search-pagination__page\"})\n",
    "maxpages = int(maxpages.get_text(strip=True).split()[-1])\n",
    "# maxpages += 5\n",
    "print(maxpages)\n",
    "\n",
    "\n",
    "for page in range(1, maxpages):\n",
    "    url = f\"https://www.ft.com/search?q={keyword}&page={page}&sort=date&isFirstView=true\"\n",
    "    result = requests.get(url, headers=headers)\n",
    "    # print(page)\n",
    "    # print(result.text)\n",
    "    if \"Sorry\" in result.text: # maybe unnecessary\n",
    "        print(f\"Stopping iteration: Page {page} is not available.\")\n",
    "        break\n",
    "    \n",
    "    result_content = result.content\n",
    "    soup = BeautifulSoup(result_content, \"lxml\")\n",
    "    \n",
    "    for article in soup.findAll(\"div\", {\"class\": \"o-teaser\"}):\n",
    "        heading = article.find(\"div\", {\"class\": \"o-teaser__heading\"}).get_text(strip=True) #find(text=True), find text is outdated\n",
    "        link = article.find(\"div\", {\"class\": \"o-teaser__heading\"}).find('a', href=True)\n",
    "        date = article.find('time', {\"class\": \"o-teaser__timestamp-date\"})\n",
    "        \n",
    "        # print(heading, link, date)\n",
    "        if heading and link and date:\n",
    "            article_date = datetime.strptime(date.text.strip(), \"%B %d, %Y\")\n",
    "            if article_date > cutoff_date:\n",
    "                news_data.append([heading, link['href'], date.text.strip()])\n",
    "\n",
    "# print(pages)\n",
    "\n",
    "print(f\"Total articles collected in news_data: {len(news_data)}\")\n",
    "# print(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup news_data, dont refresh\n",
    "\n",
    "news_data = []\n",
    "collected_links = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup keywords\n",
    "\n",
    "cutoff_date = datetime.strptime(\"01-Dec-2018\", \"%d-%b-%Y\")\n",
    "\n",
    "# keywords = [\n",
    "#     # Energy Sector\n",
    "#     \"XOM\", \"CVX\", \"EOG\", \"SLB\",\n",
    "#     \"Exxon\", \"Chevron\", \"ConocoPhillips\", \"Schlumberger\",\n",
    "#     \"oil\", \"gas\", \"energy\", \"OPEC\",\n",
    "#     \"electricity\", \"green\", \"utilities\",\n",
    "\n",
    "#     # Financials Sector\n",
    "#     \"JPM\", \"BAC\", \"WFC\", \"AXP\",\n",
    "#     \"JPMorgan\", \"Bank of America\", \"Wells Fargo\", \"American Express\", \"Morgan Stanley\",\n",
    "#     \"interest rates\", \"savings\", \"investment\", \"regulation\",\n",
    "#     \"inflation\", \"employment\", \"FED\", \"SEC\", \"NYSE\", \"NASDAQ\",\n",
    "\n",
    "#     # Tech Sector\n",
    "#     \"MSFT\", \"AAPL\", \"NVDA\", \"GOOGL\", \"META\",\n",
    "#     \"Microsoft\", \"Apple\", \"Nvidia\", \"Alphabet\", \"Meta\",\n",
    "#     \"AI\", \"Google\", \"cybersecurity\", \"fintech\", \"data\", \"cloud\"\n",
    "# ]\n",
    "\n",
    "k1 = [\n",
    "    \"XOM\", \"CVX\", \"EOG\", \"SLB\" # energy 1  \n",
    "]\n",
    "k2 = [\n",
    "    \"Exxon\", \"Chevron\", \"ConocoPhillips\", \"Schlumberger\" # energy 2\n",
    "]\n",
    "k3 = [\n",
    "    \"oil\", \"gas\", \"energy\", \"OPEC\" # energy 3\n",
    "]\n",
    "k4 = [\n",
    "    \"electricity\", \"green\", \"utilities\" # energy 4\n",
    "]\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://www.ft.com/',\n",
    "    'Origin': 'https://www.ft.com'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# ft articles data [heading, link, date] for a list of keywords, need to split the keywords list bc it crashes\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "k1 = [\n",
    "    \"XOM\", \"CVX\", \"EOG\", \"SLB\" # energy 1  \n",
    "]\n",
    "k2 = [\n",
    "    \"Exxon\", \"Chevron\", \"ConocoPhillips\", \"Schlumberger\" # energy 2\n",
    "]\n",
    "k3 = [\n",
    "    \"energy\", \"OPEC\" # energy 3 # run now oil and gas have already been done\n",
    "]\n",
    "k4 = [\n",
    "    \"utilities\" # energy 4\n",
    "]\n",
    "# collected_links = set()\n",
    "file_name = 'news_data.csv'\n",
    "\n",
    "def write_to_csv(data, file_name):\n",
    "    file_exists = os.path.isfile(file_name)\n",
    "    with open(file_name, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            # Write header only if the file does not exist\n",
    "            writer.writerow([\"Headline\", \"Link\", \"Date\"])\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def read_existing_links(file_name):\n",
    "    if not os.path.isfile(file_name):\n",
    "        return set()\n",
    "    with open(file_name, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader, None)  # Skip the header\n",
    "        existing_links = {row[1] for row in reader}\n",
    "    return existing_links\n",
    "\n",
    "existing_links = read_existing_links(file_name)\n",
    "cutoff_date = datetime.strptime(\"01-Dec-2018\", \"%d-%b-%Y\")\n",
    "\n",
    "\n",
    "for keyword in k4:\n",
    "    keyurl = f\"https://www.ft.com/search?q={keyword}&page=1&sort=date&isFirstView=true\"\n",
    "\n",
    "\n",
    "    keyresult = requests.get(keyurl, headers=headers)\n",
    "\n",
    "    keyresult_content = keyresult.content\n",
    "    keysoup = BeautifulSoup(keyresult_content, \"html.parser\")\n",
    "    try:\n",
    "        maxpages = keysoup.find('span', {\"class\": \"search-pagination__page\"})\n",
    "        maxpages = int(maxpages.get_text(strip=True).split()[-1])\n",
    "    except (AttributeError, ValueError) as e:\n",
    "        print(f\"Skipping keyword '{keyword}' due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "    for page in range(1, maxpages):\n",
    "        url = f\"https://www.ft.com/search?q={keyword}&page={page}&sort=date&isFirstView=true\"\n",
    "        result = requests.get(url, headers=headers)\n",
    "        \n",
    "        if \"Sorry\" in result.text: # maybe unnecessary\n",
    "            print(f\"Stopping iteration: Page {page} is not available.\")\n",
    "            break\n",
    "        \n",
    "        result_content = result.content\n",
    "        soup = BeautifulSoup(result_content, \"lxml\")\n",
    "        \n",
    "        skip_to_next_keyword = False\n",
    "        page_data = []\n",
    "        \n",
    "        for article in soup.findAll(\"div\", {\"class\": \"o-teaser\"}):\n",
    "            heading = article.find(\"div\", {\"class\": \"o-teaser__heading\"}).get_text(strip=True)\n",
    "            link = article.find(\"div\", {\"class\": \"o-teaser__heading\"}).find('a', href=True)\n",
    "            date = article.find('time', {\"class\": \"o-teaser__timestamp-date\"})\n",
    "            \n",
    "            if heading and link and date:\n",
    "                \n",
    "                article_date = datetime.strptime(date.text.strip(), \"%B %d, %Y\")\n",
    "                if article_date < cutoff_date:\n",
    "                    skip_to_next_keyword = True\n",
    "                    break\n",
    "                if link not in existing_links:\n",
    "                    page_data.append([heading, link, date.text.strip()])\n",
    "                    existing_links.add(link)\n",
    "        if page_data:\n",
    "            write_to_csv(page_data, file_name)\n",
    "        if skip_to_next_keyword:\n",
    "            break\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n"
     ]
    }
   ],
   "source": [
    "print(len(news_data)) #110 rows after k1, 851\n",
    "# k4 fully finished, energy 4745 rows \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Date  Count\n",
      "587      May 20, 2020      1\n",
      "588      May 17, 2020      1\n",
      "589      May 14, 2020      1\n",
      "590      May 13, 2020      1\n",
      "591      May 12, 2020      1\n",
      "..                ...    ...\n",
      "979  December 6, 2018      1\n",
      "980  December 3, 2018      1\n",
      "981    August 4, 2023      1\n",
      "982     July 24, 2023      1\n",
      "983      July 3, 2023      1\n",
      "\n",
      "[365 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "file_name = 'news_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Group by the \"Date\" column and count the occurrences\n",
    "date_counts = df['Date'].value_counts().reset_index()\n",
    "date_counts.columns = ['Date', 'Count']\n",
    "\n",
    "# Print the results\n",
    "# print(date_counts)\n",
    "\n",
    "# Sort the date_counts DataFrame by 'Count' in descending order\n",
    "date_counts_sorted = date_counts.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Get the last 365 rows\n",
    "last_365_rows = date_counts_sorted.tail(365)\n",
    "print(last_365_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries for 2019: 205\n"
     ]
    }
   ],
   "source": [
    "file_name = 'news_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Convert the 'Date' column to datetime format for processing\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%B %d, %Y')\n",
    "\n",
    "# Filter the DataFrame for dates in 2019\n",
    "df_2019 = df[df['Date'].dt.year == 2019]\n",
    "\n",
    "# Calculate the count of entries for 2019\n",
    "count_2019 = df_2019.shape[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of entries for 2019: {count_2019}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year\n",
      "2019     205\n",
      "2020     184\n",
      "2021     223\n",
      "2022     309\n",
      "2023     558\n",
      "2024    3255\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "file_name = 'news_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%B %d, %Y')\n",
    "\n",
    "# Extract the year from the 'Date' column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Filter the DataFrame for the years 2019 to 2024\n",
    "filtered_df = df[df['Year'].between(2019, 2024)]\n",
    "\n",
    "# Group by the 'Year' column and count the occurrences\n",
    "year_counts = filtered_df['Year'].value_counts().sort_index()\n",
    "\n",
    "# Print the results\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  Count\n",
      "0   2024-07-17     85\n",
      "1   2024-07-19     72\n",
      "2   2024-06-26     70\n",
      "3   2024-07-18     64\n",
      "4   2024-07-23     61\n",
      "..         ...    ...\n",
      "192 2024-01-04      1\n",
      "193 2024-02-24      1\n",
      "194 2024-01-10      1\n",
      "195 2024-02-05      1\n",
      "196 2024-03-02      1\n",
      "\n",
      "[197 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "file_name = 'news_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%B %d, %Y')\n",
    "\n",
    "# Filter the DataFrame for the year 2024\n",
    "df_2024 = df[df['Date'].dt.year == 2024]\n",
    "\n",
    "# Group by the 'Date' column and count the occurrences\n",
    "date_counts_2024 = df_2024['Date'].value_counts().reset_index()\n",
    "date_counts_2024.columns = ['Date', 'Count']\n",
    "\n",
    "# Sort the date_counts DataFrame by 'Count' in descending order\n",
    "date_counts_2024_sorted = date_counts_2024.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Get the last 365 rows\n",
    "last_365_rows_2024 = date_counts_2024_sorted.tail(365)\n",
    "\n",
    "# Print the results\n",
    "print(last_365_rows_2024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
